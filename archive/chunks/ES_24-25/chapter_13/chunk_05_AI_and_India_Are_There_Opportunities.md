# AI and India: Are There Opportunities?

AI AND INDIA: ARE THERE OPPORTUNITIES?

13.44  As India contemplates the integration of AI into its economy, the lessons of past technological revolutions underscore the critical importance of proactive institutional response. The time afforded now must be well utilised to minimise the adverse effects

40    AI  is  already  wreaking  havoc  on  Global  Power  Systems.  Bloomberg.  21st  June  2024,  https://tinyurl.com/ rdk9p3c9

41  Published in Bloomberg, 13th December 2024, https://tinyurl.com/4c66sa22

to the best of our capabilities involves equipping the workforce with future-ready skill. We must also use this time to put in place mechanisms to cushion societal impacts, a  challenge  that  resonates  deeply  with  India's  unique  demographic  and  economic landscape.

13.45  Looking ahead, the nation's predominantly services-driven economy, coupled with its young and dynamic population, offers a fertile ground for leveraging the benefits of emerging technologies, only if proactively and carefully managed. Technology does not always have to displace labour but instead can be put to use in augmenting the productivity of the workforce. Just as history provides a reason for caution, history also provides a cause for optimism about the effectiveness of strong institutions which can foster an environment where man and machine work together.

13.46  Further, the exposure of medium- to high-skill jobs to AI driven automation may not be as high as certain estimates, due to the inherent limitations of AI as detailed in Box XIII.1. Thus, the labour augmenting potential of AI should also not be ignored.

## Box XIII.1: Demystifying Artificial Intelligence

In the simplest terms, the 'AI' tools on the market today, particularly Generative AI, are statistical models, utilising significant computing power, that are a function of large amounts of text, images, and other forms of data fed into them. The processing of any input is broken down across many layers for the most complex models, with each layer containing several thousand nodes (or neurons 42 ). This combination of layers and nodes allows the model to 'think', 'reason', and process data at unimaginable scales, generating an output along the parameters the model has been trained for.

When you ask any modern chatbot driven by an underlying large language model a simple question  such  as  'Where  does  the  sun  rise  and  set?',  the  model  does  not  interpret  the language in the question as an actual human does. This is because 'AI' has no understanding of the concepts of letters and syllables. The machine processes input text using a series of mathematical computations involving matrices. First, the text is broken into smaller units called tokens in a process known as tokenisation, where each token is mapped to a unique number. For example, the phrase 'AI revolution' might be tokenised to [342, 2591], where these numbers correspond to the indices of the words in the model's vocabulary.

Once  tokenised,  the  model  uses  mechanisms  to  compute  the  frequency  of  token  pairs, triplets,  and  other  sequences  between  each  word  and  all  other  words  in  the  input.  This allows the model to assign importance (weights) to different words based on their context.

42    In  the  1940s,  Warren McCulloch and Walter Pitts developed a mathematical model to mimic how the brain processes information. They proposed that neurons in the brain function like switches, turning "on" or "off" based on signals they receive from other neurons. Their model used simple logic, where a neuron activates if the  sum of incoming signals surpasses a certain threshold, similar to binary decision-making in computers. This foundational concept inspired the development of artificial neural networks, computer systems designed to emulate the brain's signal processing for tasks like pattern recognition, decision-making, and problem-solving.

These weighted representations are passed through layers of the neural network, where patterns and relationships are refined. The network output assigns probability values to each possible token, representing how likely each one will appear next. The tokens with the highest probabilities are selected and mapped back to their corresponding words in the vocabulary, generating a coherent text output. The new tokens generated are then fed back into the model, making it appear that the bot is equipped to have a flowing conversation with the user.

Thus, to respond to a user's query, the data analysis performed by the model is essentially a  game  of  'guess  the  next  word'.  In  other  words,  it  is  a  highly  complex  version  of  the autocomplete function we already see on our computers and smartphones. Generative AI are trained to simply predict the next word in a sequence of words by calculating probabilities based on the user input text. Considering the non-linearity and complexity of language, such an exercise is computationally very expensive. To generate a single-line response to the user's question (in our example, 'The sun rises in the east and sets in the west'), the model may need to perform anywhere between 10 to 20 trillion arithmetic operations to generate a 11-word response. Similar principles govern the functioning of other generative AI.

Strides made in AI-research is awe-inspiring and will most likely be helpful in the coming years 43 .  However, Michael Wooldridge from the University of Oxford had suggested that claims of intelligence required more rigorous scrutiny, stating that large language models, despite their dazzling appearance of human-like competence, are not 'AI' 44 . While capable of some superficial logical reasoning and problem-solving, these models are limited in their extended capabilities. Anything additional expected from these models must be explicitly coded into them, which is very different from what is traditionally considered 'intelligent'. To claim that machines are 'learning' is to assign the wrong label since these models use predictive and probabilistic statistics to generate an output.

Consider  an  AI-powered  marketing  tool  that  determines  campaign  success  based  solely on click-through rates and conversions. While efficient, it might ignore brand perception, customer loyalty, and the long-term impact of the campaign on the business. In education, implementing  an  AI-based  grading  system  for  students  that  evaluates  essays  based  on grammar, structure, and word count can be quick and efficient. But the AI may miss the value brought by creativity, originality, and critical thinking expressed in the content.

Along similar lines, AI in healthcare can recommend treatments that prioritise statistical outcomes  since  it  cannot  factor  parameters  such  as  quality  of  life,  patient  preferences, and  ethical  considerations.  Relying  on  AI  for  judicial  decisions  also  involves  risks  since predicting recidivism or determining bail requires balancing subjective considerations such as fairness, individual circumstances, and social impact. Judgements passed in courts are much more than simple prediction tasks and are a product of personal experience combined with domain-specific knowledge, the former of which AI lacks.

43    However,  the  efficiency  of  the  models  is  still  a  research  question  that  remains  unanswered  thus  far  in  the Generative AI community. Larger models drive up the demand for more computational resources and energy, which in turn drives up the cost of running these models.

44  ChatGPT is not 'true AI'. Michael Wooldridge, 2023,  https://tinyurl.com/45s6dkad

Advancements in computer science may just as well address these concerns in the future. However, in the meantime, just as machines are designed for specific  tasks  rather  than universal application, AI functions as a tool tailored to particular purposes. This means it is  more suited to supplement human action rather than be a total replacement for work performed by them.

##