{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "80cd9c84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import re\n",
    "from pathlib import Path\n",
    "from typing import Dict, List\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from langchain_groq import ChatGroq\n",
    "from pydantic import BaseModel, Field\n",
    "import time\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c0a17d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize LLM\n",
    "llm = ChatGroq(\n",
    "    model=\"meta-llama/llama-4-scout-17b-16e-instruct\",\n",
    "    temperature=0.1,\n",
    ")\n",
    "\n",
    "\n",
    "# Pydantic Models\n",
    "class ChapterSummary(BaseModel):\n",
    "    summary: str = Field(\n",
    "        ...,\n",
    "        description=\"Comprehensive summary of the chapter retaining all facts and figures\",\n",
    "    )\n",
    "\n",
    "\n",
    "class SubchapterTags(BaseModel):\n",
    "    tags: List[str] = Field(\n",
    "        ..., description=\"List of 1-3 relevant tags for knowledge graph\", max_length=3\n",
    "    )\n",
    "\n",
    "\n",
    "class ChunkOutput(BaseModel):\n",
    "    chapter_no: int = Field(..., description=\"Chapter number\")\n",
    "    subchapter_no: int = Field(..., description=\"Subchapter/chunk number\")\n",
    "    content: str = Field(..., description=\"Chapter summary + subchapter content\")\n",
    "    tags: List[str] = Field(..., description=\"Generated tags for the subchapter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a3b18799",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file_content(file_path: str) -> str:\n",
    "    \"\"\"Read content from a markdown file\"\"\"\n",
    "    try:\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            return file.read()\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading file {file_path}: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "def extract_chapter_number(filename: str) -> int:\n",
    "    \"\"\"Extract chapter number from filename\"\"\"\n",
    "    match = re.search(r\"chapter_(\\d+)\", filename)\n",
    "    return int(match.group(1)) if match else 0\n",
    "\n",
    "\n",
    "def extract_chunk_number(filename: str) -> int:\n",
    "    \"\"\"Extract chunk number from filename\"\"\"\n",
    "    match = re.search(r\"chunk_(\\d+)\", filename)\n",
    "    return int(match.group(1)) if match else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "368fafa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_chapter_summary(chapter_content: str) -> str:\n",
    "    \"\"\"Generate summary with error handling for 413, 400, 429 using recursion.\"\"\"\n",
    "\n",
    "    structured_llm = llm.with_structured_output(ChapterSummary)\n",
    "\n",
    "    prompt = f\"\"\"You are tasked with creating a comprehensive summary of an economic survey chapter.\n",
    "\n",
    "    Instructions:\n",
    "    - Retain ALL numerical data, statistics, percentages, and figures mentioned\n",
    "    - Include key policy recommendations and findings  \n",
    "    - Maintain the factual accuracy of economic indicators\n",
    "    - Keep the summary detailed enough to understand the chapter's main points\n",
    "    - Focus on economic trends, challenges, and outlook presented\n",
    "\n",
    "    Chapter Content:\n",
    "    {chapter_content}\n",
    "\n",
    "    Generate a detailed summary that preserves all important facts and figures.\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        result = structured_llm.invoke(prompt)\n",
    "        return result.summary\n",
    "\n",
    "    except Exception as e:\n",
    "        # Parse Groq error JSON if possible\n",
    "        try:\n",
    "            err_obj = json.loads(str(e).split(\"Error code:\", 1)[-1].strip())\n",
    "        except Exception:\n",
    "            err_obj = {\"error\": {\"message\": str(e)}}\n",
    "\n",
    "        error_code = err_obj.get(\"error\", {}).get(\"code\", \"\")\n",
    "        error_msg = err_obj.get(\"error\", {}).get(\"message\", \"\")\n",
    "\n",
    "        MAX_CHARS_SAFE = 30000\n",
    "\n",
    "        # --- Handle 413: Request too large ---\n",
    "        if \"413\" in str(e) or \"Request too large\" in error_msg:\n",
    "            print(\"⚠️ 413 error: input too large. Trimming content and retrying...\")\n",
    "            trimmed_content = chapter_content[:MAX_CHARS_SAFE]\n",
    "            return generate_chapter_summary(trimmed_content)\n",
    "\n",
    "        # --- Handle 400: tool_use_failed, return failed_generation ---\n",
    "        if \"400\" in str(e) or \"tool_use_failed\" in error_code:\n",
    "            failed_text = err_obj.get(\"error\", {}).get(\"failed_generation\")\n",
    "            if failed_text:\n",
    "                print(\"⚠️ 400 error: returning failed_generation output.\")\n",
    "                return failed_text\n",
    "            return f\"Summary generation failed with 400: {error_msg}\"\n",
    "\n",
    "        # --- Handle 429: Rate limit exceeded ---\n",
    "        if \"429\" in str(e) or \"rate_limit\" in error_code:\n",
    "            print(\"⚠️ 429 error: rate limit exceeded. Waiting 60s before retry...\")\n",
    "            time.sleep(60)\n",
    "            return generate_chapter_summary(chapter_content)\n",
    "\n",
    "        # --- Other errors ---\n",
    "        print(f\"❌ Unhandled error: {error_msg}\")\n",
    "        return f\"Summary generation failed: {error_msg}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d196cc9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_subchapter_tags(subchapter_content: str) -> List[str]:\n",
    "    \"\"\"Generate tags for a subchapter\"\"\"\n",
    "    structured_llm = llm.with_structured_output(SubchapterTags)\n",
    "\n",
    "    prompt = f\"\"\"Analyze the provided economic survey subchapter content and generate 1-3 relevant tags.\n",
    "\n",
    "    Instructions:\n",
    "    - Tags should be suitable for a knowledge graph\n",
    "    - Focus on main economic themes, sectors, or concepts\n",
    "    - Use concise, descriptive terms (e.g., \"GDP_Growth\", \"Inflation\", \"Manufacturing\", \"Trade_Policy\")\n",
    "    - Avoid generic tags like \"economics\" or \"data\"\n",
    "    - Prioritize the most specific and relevant topics\n",
    "\n",
    "    Subchapter Content:\n",
    "    {subchapter_content}\n",
    "\n",
    "    Generate up to 3 specific tags that best represent this content for knowledge graph categorization.\"\"\"\n",
    "\n",
    "    try:\n",
    "        result = structured_llm.invoke(prompt)\n",
    "        return result.tags\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating tags: {e}\")\n",
    "        return [\"processing_error\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8621659e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def discover_chapters_and_chunks(base_path: str) -> Dict:\n",
    "    \"\"\"Discover all chapters and their chunks\"\"\"\n",
    "    base_path = Path(base_path)\n",
    "    chapters_data = {}\n",
    "\n",
    "    # Find all chapter directories\n",
    "    chapter_dirs = [\n",
    "        d for d in base_path.iterdir() if d.is_dir() and d.name.startswith(\"chapter_\")\n",
    "    ]\n",
    "    chapter_dirs.sort(key=lambda x: extract_chapter_number(x.name))\n",
    "\n",
    "    for chapter_dir in chapter_dirs:\n",
    "        chapter_no = extract_chapter_number(chapter_dir.name)\n",
    "\n",
    "        # Find main chapter file\n",
    "        chapter_file = chapter_dir / f\"chapter_{chapter_no}.md\"\n",
    "\n",
    "        # Find chunk files in this chapter\n",
    "        chunk_files = [\n",
    "            f\n",
    "            for f in chapter_dir.iterdir()\n",
    "            if f.is_file() and f.name.startswith(\"chunk_\") and f.name.endswith(\".md\")\n",
    "        ]\n",
    "        chunk_files.sort(key=lambda x: extract_chunk_number(x.name))\n",
    "\n",
    "        chapters_data[chapter_no] = {\n",
    "            \"chapter_file\": str(chapter_file) if chapter_file.exists() else None,\n",
    "            \"chunk_files\": [\n",
    "                (extract_chunk_number(f.name), str(f)) for f in chunk_files\n",
    "            ],\n",
    "        }\n",
    "\n",
    "    total_chunks = sum(len(data[\"chunk_files\"]) for data in chapters_data.values())\n",
    "    print(f\"Discovered {total_chunks} chunks across {len(chapters_data)} chapters\")\n",
    "\n",
    "    return chapters_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "531b1704",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_chapter_summary(chapter_file_path: str, chapter_no: int) -> str:\n",
    "    \"\"\"Process and generate summary for a chapter\"\"\"\n",
    "    if not chapter_file_path or not os.path.exists(chapter_file_path):\n",
    "        print(f\"Warning: Chapter {chapter_no} file not found at {chapter_file_path}\")\n",
    "        return f\"Chapter {chapter_no} Summary (file not found)\"\n",
    "\n",
    "    print(f\"Generating summary for Chapter {chapter_no}...\")\n",
    "    chapter_content = read_file_content(chapter_file_path)\n",
    "\n",
    "    if chapter_content:\n",
    "        chapter_summary = generate_chapter_summary(chapter_content)\n",
    "        print(\n",
    "            f\"Generated summary for Chapter {chapter_no} ({len(chapter_summary)} chars)\"\n",
    "        )\n",
    "        return chapter_summary\n",
    "    else:\n",
    "        return f\"Chapter {chapter_no} Summary (content not readable)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6ee2caa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_chunk(\n",
    "    chunk_no: int, chunk_file_path: str, chapter_no: int, chapter_summary: str\n",
    ") -> Dict:\n",
    "    \"\"\"Process a single chunk\"\"\"\n",
    "    print(f\"Processing Chapter {chapter_no}, Chunk {chunk_no}...\")\n",
    "\n",
    "    # Read chunk content\n",
    "    chunk_content = read_file_content(chunk_file_path)\n",
    "\n",
    "    if not chunk_content:\n",
    "        print(f\"Warning: Could not read chunk file {chunk_file_path}\")\n",
    "        return None\n",
    "\n",
    "    # Generate tags for this subchapter\n",
    "    tags = generate_subchapter_tags(chunk_content)\n",
    "\n",
    "    # Combine chapter summary with chunk content\n",
    "    combined_content = (\n",
    "        f\"Chapter {chapter_no} Summary:\\n{chapter_summary}\\n\\n\"\n",
    "        f\"Subchapter {chunk_no} Content:\\n{chunk_content}\"\n",
    "    )\n",
    "\n",
    "    # Create chunk output\n",
    "    chunk_output = {\n",
    "        \"chapter_no\": chapter_no,\n",
    "        \"subchapter_no\": chunk_no,\n",
    "        \"content\": combined_content,\n",
    "        \"tags\": tags,\n",
    "    }\n",
    "\n",
    "    print(f\"Processed chunk {chunk_no} with tags: {tags}\")\n",
    "    return chunk_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "87d87826",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_chunk_as_json(chunk_data: Dict, output_dir: str) -> None:\n",
    "    \"\"\"Save a single chunk as JSON file\"\"\"\n",
    "    output_dir = Path(output_dir)\n",
    "    output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "    filename = (\n",
    "        f\"chapter_{chunk_data['chapter_no']}_chunk_{chunk_data['subchapter_no']}.json\"\n",
    "    )\n",
    "    file_path = output_dir / filename\n",
    "\n",
    "    try:\n",
    "        with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(chunk_data, f, indent=2, ensure_ascii=False)\n",
    "        print(f\"Saved: {filename}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving {filename}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ecb0af75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_documents(base_path: str, output_dir: str = \"processed_chunks\") -> Dict:\n",
    "    \"\"\"Main function to process all documents\"\"\"\n",
    "    print(\"Starting document processing...\")\n",
    "\n",
    "    # Discover all chapters and chunks\n",
    "    chapters_data = discover_chapters_and_chunks(base_path)\n",
    "\n",
    "    if not chapters_data:\n",
    "        print(\"No chapters found in the specified path\")\n",
    "        return {}\n",
    "\n",
    "    # Process each chapter\n",
    "    chapter_summaries = {}\n",
    "    processed_chunks = []\n",
    "\n",
    "    for chapter_no in sorted(chapters_data.keys()):\n",
    "        chapter_info = chapters_data[chapter_no]\n",
    "\n",
    "        # Generate chapter summary if not already done\n",
    "        if chapter_no not in chapter_summaries:\n",
    "            chapter_summaries[chapter_no] = process_chapter_summary(\n",
    "                chapter_info[\"chapter_file\"], chapter_no\n",
    "            )\n",
    "\n",
    "        # Process each chunk in this chapter\n",
    "        for chunk_no, chunk_file_path in chapter_info[\"chunk_files\"]:\n",
    "            chunk_data = process_chunk(\n",
    "                chunk_no, chunk_file_path, chapter_no, chapter_summaries[chapter_no]\n",
    "            )\n",
    "\n",
    "            if chunk_data:\n",
    "                # Save immediately to avoid memory issues with large datasets\n",
    "                save_chunk_as_json(chunk_data, output_dir)\n",
    "                processed_chunks.append(\n",
    "                    {\"chapter_no\": chapter_no, \"chunk_no\": chunk_no, \"file_saved\": True}\n",
    "                )\n",
    "\n",
    "    print(\"\\nDocument processing completed!\")\n",
    "    print(\n",
    "        f\"Processed {len(processed_chunks)} chunks from {len(chapters_data)} chapters\"\n",
    "    )\n",
    "    print(f\"Output saved to: {output_dir}\")\n",
    "\n",
    "    return {\n",
    "        \"total_chapters\": len(chapters_data),\n",
    "        \"total_chunks\": len(processed_chunks),\n",
    "        \"output_directory\": output_dir,\n",
    "        \"processed_chunks\": processed_chunks,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1e77f6e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_processing_summary(result: Dict, output_dir: str) -> None:\n",
    "    \"\"\"Generate a summary of processing results\"\"\"\n",
    "    summary_file = Path(output_dir) / \"processing_summary.json\"\n",
    "\n",
    "    try:\n",
    "        with open(summary_file, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(result, f, indent=2, ensure_ascii=False)\n",
    "        print(f\"Processing summary saved to: {summary_file}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving processing summary: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "99dc10d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting document processing...\n",
      "Discovered 94 chunks across 13 chapters\n",
      "Generating summary for Chapter 1...\n",
      "Generated summary for Chapter 1 (6942 chars)\n",
      "Processing Chapter 1, Chunk 1...\n",
      "Processed chunk 1 with tags: ['GDP_Growth', 'Global_Trade', 'Inflation_Pressures']\n",
      "Saved: chapter_1_chunk_1.json\n",
      "Processing Chapter 1, Chunk 2...\n",
      "Processed chunk 2 with tags: ['Global_Economic_Conditions', 'Macroeconomic_Trends', 'Growth_and_Inflation']\n",
      "Saved: chapter_1_chunk_2.json\n",
      "Processing Chapter 1, Chunk 3...\n",
      "Processed chunk 3 with tags: ['Global_Economic_Growth', 'Inflation_Rates', 'Trade_Policy_Uncertainty']\n",
      "Saved: chapter_1_chunk_3.json\n",
      "Processing Chapter 1, Chunk 4...\n",
      "Processed chunk 4 with tags: ['GDP_Growth', 'Agricultural_Recovery', 'Manufacturing_Sector']\n",
      "Saved: chapter_1_chunk_4.json\n",
      "Processing Chapter 1, Chunk 5...\n",
      "Processed chunk 5 with tags: ['Fiscal_Discipline', 'Economic_Stability', 'Inflation_Control']\n",
      "Saved: chapter_1_chunk_5.json\n",
      "Processing Chapter 1, Chunk 6...\n",
      "Processed chunk 6 with tags: ['Global_Economic_Outlook', 'Inflation_Pressures', 'Manufacturing_Resilience']\n",
      "Saved: chapter_1_chunk_6.json\n",
      "Generating summary for Chapter 2...\n",
      "⚠️ 413 error: input too large. Trimming content and retrying...\n",
      "Generated summary for Chapter 2 (3085 chars)\n",
      "Processing Chapter 2, Chunk 1...\n",
      "Processed chunk 1 with tags: ['Monetary_Policy', 'Financial_Intermediation', 'Economic_Growth']\n",
      "Saved: chapter_2_chunk_1.json\n",
      "Processing Chapter 2, Chunk 2...\n",
      "Processed chunk 2 with tags: ['Monetary_Policy', 'Money_Supply', 'Financial_Stability']\n",
      "Saved: chapter_2_chunk_2.json\n",
      "Processing Chapter 2, Chunk 3...\n",
      "Processed chunk 3 with tags: ['FinancialIntermediation', 'BankingSector', 'CreditGrowth']\n",
      "Saved: chapter_2_chunk_3.json\n",
      "Processing Chapter 2, Chunk 4...\n",
      "Processed chunk 4 with tags: ['Financial_Sector_Risks', 'Financial_Inclusion', 'Regulatory_Quality']\n",
      "Saved: chapter_2_chunk_4.json\n",
      "Processing Chapter 2, Chunk 5...\n",
      "Processed chunk 5 with tags: ['Financial_Sector_Outlook', 'India_Economic_Growth', 'Financialisation_Risks']\n",
      "Saved: chapter_2_chunk_5.json\n",
      "Generating summary for Chapter 3...\n",
      "Generated summary for Chapter 3 (6099 chars)\n",
      "Processing Chapter 3, Chunk 1...\n",
      "Processed chunk 1 with tags: ['FDI', 'Trade_Policy', 'Export_Growth']\n",
      "Saved: chapter_3_chunk_1.json\n",
      "Processing Chapter 3, Chunk 2...\n",
      "Processed chunk 2 with tags: ['Trade_Policy_Uncertainty', 'Geopolitical_Risk', 'Economic_Policy_Uncertainty']\n",
      "Saved: chapter_3_chunk_2.json\n",
      "Processing Chapter 3, Chunk 3...\n",
      "Processed chunk 3 with tags: ['Global_Trade_Dynamics', 'Trade_Policy', 'Non_Tariff_Measures']\n",
      "Saved: chapter_3_chunk_3.json\n",
      "Processing Chapter 3, Chunk 4...\n",
      "Processed chunk 4 with tags: ['Trade_Performance', 'Textile_Exports', 'E-commerce_Exports']\n",
      "Saved: chapter_3_chunk_4.json\n",
      "Processing Chapter 3, Chunk 5...\n",
      "Processed chunk 5 with tags: ['Trade_Facilitation', 'Export_Competitiveness', 'Logistics_Efficiency']\n",
      "Saved: chapter_3_chunk_5.json\n",
      "Processing Chapter 3, Chunk 6...\n",
      "Processed chunk 6 with tags: ['Balance_of_Payments', 'Foreign_Investment', 'Economic_Resilience']\n",
      "Saved: chapter_3_chunk_6.json\n",
      "Processing Chapter 3, Chunk 7...\n",
      "Processed chunk 7 with tags: ['Trade_Competitiveness', 'Global_Trade_Dynamics', 'Economic_Growth_Strategy']\n",
      "Saved: chapter_3_chunk_7.json\n",
      "Generating summary for Chapter 4...\n",
      "Generated summary for Chapter 4 (1054 chars)\n",
      "Processing Chapter 4, Chunk 1...\n",
      "Processed chunk 1 with tags: ['Inflation_Dynamics', 'Global_Inflation', 'Monetary_Policy']\n",
      "Saved: chapter_4_chunk_1.json\n",
      "Processing Chapter 4, Chunk 2...\n",
      "Processed chunk 2 with tags: ['Inflation_Rate', 'Monetary_Policy', 'Global_Economy']\n",
      "Saved: chapter_4_chunk_2.json\n",
      "Processing Chapter 4, Chunk 3...\n",
      "Processed chunk 3 with tags: ['Food_Inflation', 'Vegetable_Prices', 'Weather_Impact']\n",
      "Saved: chapter_4_chunk_3.json\n",
      "Processing Chapter 4, Chunk 4...\n",
      "Processed chunk 4 with tags: ['Inflation_Outlook', 'Agricultural_Production', 'Commodity_Prices']\n",
      "Saved: chapter_4_chunk_4.json\n",
      "Generating summary for Chapter 5...\n",
      "Generated summary for Chapter 5 (5434 chars)\n",
      "Processing Chapter 5, Chunk 1...\n",
      "Processed chunk 1 with tags: ['GDP_Growth', 'Geo_Economic_Fragmentation', 'Trade_Policy']\n",
      "Saved: chapter_5_chunk_1.json\n",
      "Processing Chapter 5, Chunk 2...\n",
      "Processed chunk 2 with tags: ['Manufacturing_Dominance', 'Global_Trade_Shift', 'Economic_Fragmentation']\n",
      "Saved: chapter_5_chunk_2.json\n",
      "Processing Chapter 5, Chunk 3...\n",
      "Processed chunk 3 with tags: ['Climate_Transition', 'Renewable_Energy', 'Trade_Policy']\n",
      "Saved: chapter_5_chunk_3.json\n",
      "Processing Chapter 5, Chunk 4...\n",
      "Processed chunk 4 with tags: ['Deregulation', 'Economic Growth', 'MSME Development']\n",
      "Saved: chapter_5_chunk_4.json\n",
      "Generating summary for Chapter 6...\n",
      "Generated summary for Chapter 6 (952 chars)\n",
      "Processing Chapter 6, Chunk 1...\n",
      "Processed chunk 1 with tags: ['Infrastructure_Investment', 'Economic_Development', 'Private_Participation']\n",
      "Saved: chapter_6_chunk_1.json\n",
      "Processing Chapter 6, Chunk 2...\n",
      "Processed chunk 2 with tags: ['Infrastructure_Capex', 'Government_Expenditure', 'Economic_Recovery']\n",
      "Saved: chapter_6_chunk_2.json\n",
      "Processing Chapter 6, Chunk 3...\n",
      "Processed chunk 3 with tags: ['Railway_Expansion', 'Infrastructure_Development', 'Transportation_Network']\n",
      "Saved: chapter_6_chunk_3.json\n",
      "Processing Chapter 6, Chunk 4...\n",
      "Processed chunk 4 with tags: ['Renewable_Energy', 'Power_Sector', 'Energy_Infrastructure']\n",
      "Saved: chapter_6_chunk_4.json\n",
      "Processing Chapter 6, Chunk 5...\n",
      "Processed chunk 5 with tags: ['Digital_Connectivity', 'Telecommunications_Infrastructure', '5G_Network_Expansion']\n",
      "Saved: chapter_6_chunk_5.json\n",
      "Processing Chapter 6, Chunk 6...\n",
      "Processed chunk 6 with tags: ['Rural_Development', 'Water_Security', 'Waste_Management']\n",
      "Saved: chapter_6_chunk_6.json\n",
      "Processing Chapter 6, Chunk 7...\n",
      "Processed chunk 7 with tags: ['Urban_Development', 'Sustainable_Infrastructure', 'Smart_Cities']\n",
      "Saved: chapter_6_chunk_7.json\n",
      "Processing Chapter 6, Chunk 8...\n",
      "Processed chunk 8 with tags: ['Domestic_Tourism', 'Tourism_Development', 'Infrastructure_Investment']\n",
      "Saved: chapter_6_chunk_8.json\n",
      "Processing Chapter 6, Chunk 9...\n",
      "Processed chunk 9 with tags: ['Space_Infrastructure', 'Geospatial_Technology', 'Satellite_Applications']\n",
      "Saved: chapter_6_chunk_9.json\n",
      "Processing Chapter 6, Chunk 10...\n",
      "Processed chunk 10 with tags: ['Infrastructure_Development', 'Public_Private_Partnerships', 'Sustainable_Growth']\n",
      "Saved: chapter_6_chunk_10.json\n",
      "Generating summary for Chapter 7...\n",
      "Generated summary for Chapter 7 (4973 chars)\n",
      "Processing Chapter 7, Chunk 1...\n",
      "Processed chunk 1 with tags: ['Manufacturing', 'Business_Reforms', 'Industrial_Development']\n",
      "Saved: chapter_7_chunk_1.json\n",
      "Processing Chapter 7, Chunk 2...\n",
      "Processed chunk 2 with tags: ['Manufacturing', 'Industrialization', 'Global_Economy']\n",
      "Saved: chapter_7_chunk_2.json\n",
      "Processing Chapter 7, Chunk 3...\n",
      "Processed chunk 3 with tags: ['Industrial_Growth', 'Manufacturing_Sector', 'Economic_Recovery']\n",
      "Saved: chapter_7_chunk_3.json\n",
      "Processing Chapter 7, Chunk 4...\n",
      "Processed chunk 4 with tags: ['Cement_Production', 'Steel_Industry', 'Chemical_Sector']\n",
      "Saved: chapter_7_chunk_4.json\n",
      "Processing Chapter 7, Chunk 5...\n",
      "Processed chunk 5 with tags: ['Manufacturing', 'MSME', 'R&D']\n",
      "Saved: chapter_7_chunk_5.json\n",
      "Processing Chapter 7, Chunk 6...\n",
      "Processed chunk 6 with tags: ['Industrial_Production', 'State_Economic_Development', 'Manufacturing_Growth']\n",
      "Saved: chapter_7_chunk_6.json\n",
      "Processing Chapter 7, Chunk 7...\n",
      "Processed chunk 7 with tags: ['Manufacturing_Competitiveness', 'Industrial_Policy', 'Global_Trade_Challenges']\n",
      "Saved: chapter_7_chunk_7.json\n",
      "Generating summary for Chapter 8...\n",
      "Generated summary for Chapter 8 (11694 chars)\n",
      "Processing Chapter 8, Chunk 1...\n",
      "Processed chunk 1 with tags: ['Service_Sector_Growth', 'Digital_Transformation', 'India_Economy']\n",
      "Saved: chapter_8_chunk_1.json\n",
      "Processing Chapter 8, Chunk 2...\n",
      "Processed chunk 2 with tags: ['Services_Economy', 'Global_GDP', 'Trade_Exports']\n",
      "Saved: chapter_8_chunk_2.json\n",
      "Processing Chapter 8, Chunk 3...\n",
      "Processed chunk 3 with tags: ['Services_Sector_Performance', 'India_Economy', 'GDP_Contribution']\n",
      "Saved: chapter_8_chunk_3.json\n",
      "Processing Chapter 8, Chunk 4...\n",
      "Processed chunk 4 with tags: ['Bank_Credit', 'FDI_Inflows', 'Services_Sector']\n",
      "Saved: chapter_8_chunk_4.json\n",
      "Processing Chapter 8, Chunk 5...\n",
      "Processed chunk 5 with tags: ['Tourism_Recovery', 'Real_Estate_Growth', 'GDP_Contribution']\n",
      "Saved: chapter_8_chunk_5.json\n",
      "Processing Chapter 8, Chunk 6...\n",
      "Processed chunk 6 with tags: ['IT_services', 'GCCs', 'Telecom_sector']\n",
      "Saved: chapter_8_chunk_6.json\n",
      "Processing Chapter 8, Chunk 7...\n",
      "Processed chunk 7 with tags: ['Service_Sector_Performance', 'State_Wise_Analysis', 'Economic_Dispersion']\n",
      "Saved: chapter_8_chunk_7.json\n",
      "Processing Chapter 8, Chunk 8...\n",
      "Processed chunk 8 with tags: ['Service_Sector_Growth', 'Digital_Transformation', 'Skilling_and_Resilience']\n",
      "Saved: chapter_8_chunk_8.json\n",
      "Generating summary for Chapter 9...\n",
      "Generated summary for Chapter 9 (2257 chars)\n",
      "Processing Chapter 9, Chunk 1...\n",
      "Processed chunk 1 with tags: ['Agricultural_Productivity', 'Climate_Resilience', 'Food_Security']\n",
      "Saved: chapter_9_chunk_1.json\n",
      "Processing Chapter 9, Chunk 2...\n",
      "Processed chunk 2 with tags: ['Agricultural_Growth', 'Indian_Economy', 'Sustainable_Agriculture']\n",
      "Saved: chapter_9_chunk_2.json\n",
      "Processing Chapter 9, Chunk 3...\n",
      "Processed chunk 3 with tags: ['Crop_Productivity', 'Agricultural_Policies', 'Sustainable_Farming']\n",
      "Saved: chapter_9_chunk_3.json\n",
      "Processing Chapter 9, Chunk 4...\n",
      "Processed chunk 4 with tags: ['Sustainable_Agriculture', 'Soil_Health', 'Climate_Resilient_Seeds']\n",
      "Saved: chapter_9_chunk_4.json\n",
      "Processing Chapter 9, Chunk 5...\n",
      "Processed chunk 5 with tags: ['Irrigation_Systems', 'Climate_Change_Impact', 'Agricultural_Productivity']\n",
      "Saved: chapter_9_chunk_5.json\n",
      "Processing Chapter 9, Chunk 6...\n",
      "Processed chunk 6 with tags: ['Agriculture Credit', 'Kisan Credit Card', 'Farmers Insurance']\n",
      "Saved: chapter_9_chunk_6.json\n",
      "Processing Chapter 9, Chunk 7...\n",
      "Processed chunk 7 with tags: ['Agricultural_Mechanisation', 'Farm_Equipment_Access', 'Sustainable_Agriculture']\n",
      "Saved: chapter_9_chunk_7.json\n",
      "Processing Chapter 9, Chunk 8...\n",
      "Processed chunk 8 with tags: ['Agricultural_Extension', 'Sustainable_Agriculture', 'Farmer_Training']\n",
      "Saved: chapter_9_chunk_8.json\n",
      "Processing Chapter 9, Chunk 9...\n",
      "Processed chunk 9 with tags: ['Agricultural_Infrastructure', 'Farm_Marketing_Schemes', 'Rural_Development']\n",
      "Saved: chapter_9_chunk_9.json\n",
      "Processing Chapter 9, Chunk 10...\n",
      "Processed chunk 10 with tags: ['Sustainable_Agriculture', 'Climate_Resilience', 'Organic_Farming']\n",
      "Saved: chapter_9_chunk_10.json\n",
      "Processing Chapter 9, Chunk 11...\n",
      "Processed chunk 11 with tags: ['Livestock_Economy', 'Fisheries_Development', 'Cooperative_Societies']\n",
      "Saved: chapter_9_chunk_11.json\n",
      "Processing Chapter 9, Chunk 12...\n",
      "Processed chunk 12 with tags: ['Food_Processing_Industry', 'Economic_Development', 'Agri_Food_Exports']\n",
      "Saved: chapter_9_chunk_12.json\n",
      "Processing Chapter 9, Chunk 13...\n",
      "Processed chunk 13 with tags: ['Food_Security', 'Public_Distribution_System', 'Agricultural_Storage_Infrastructure']\n",
      "Saved: chapter_9_chunk_13.json\n",
      "Processing Chapter 9, Chunk 14...\n",
      "Processed chunk 14 with tags: ['Agricultural_Sector', 'Sustainable_Agriculture', 'Food_Security']\n",
      "Saved: chapter_9_chunk_14.json\n",
      "Generating summary for Chapter 10...\n",
      "Generated summary for Chapter 10 (1867 chars)\n",
      "Processing Chapter 10, Chunk 1...\n",
      "Processed chunk 1 with tags: ['Sustainable_Development', 'Climate_Adaptation', 'Low_Carbon_Growth']\n",
      "Saved: chapter_10_chunk_1.json\n",
      "Processing Chapter 10, Chunk 2...\n",
      "Processed chunk 2 with tags: ['Low_Carbon_Development', 'Climate_Change_Adaptation', 'Sustainable_Economy']\n",
      "Saved: chapter_10_chunk_2.json\n",
      "Processing Chapter 10, Chunk 3...\n",
      "Processed chunk 3 with tags: ['Climate_Adaptation', 'Sustainable_Development', 'Water_Management']\n",
      "Saved: chapter_10_chunk_3.json\n",
      "Processing Chapter 10, Chunk 4...\n",
      "Processed chunk 4 with tags: ['Energy_Transition', 'Fossil_Fuels', 'Renewable_Energy']\n",
      "Saved: chapter_10_chunk_4.json\n",
      "Processing Chapter 10, Chunk 5...\n",
      "Processed chunk 5 with tags: ['RenewableEnergy', 'EnergyTransition', 'SustainableDevelopment']\n",
      "Saved: chapter_10_chunk_5.json\n",
      "Processing Chapter 10, Chunk 6...\n",
      "Processed chunk 6 with tags: ['Sustainable_Development', 'Lifestyle_for_Environment', 'Circular_Economy']\n",
      "Saved: chapter_10_chunk_6.json\n",
      "Processing Chapter 10, Chunk 7...\n",
      "Processed chunk 7 with tags: ['Air_Pollution', 'Environmental_Policy', 'Public_Health']\n",
      "Saved: chapter_10_chunk_7.json\n",
      "Processing Chapter 10, Chunk 8...\n",
      "Processed chunk 8 with tags: ['LowCarbonDevelopment', 'ClimateResilience', 'RenewableEnergy']\n",
      "Saved: chapter_10_chunk_8.json\n",
      "Generating summary for Chapter 11...\n",
      "⚠️ 413 error: input too large. Trimming content and retrying...\n",
      "Generated summary for Chapter 11 (1877 chars)\n",
      "Processing Chapter 11, Chunk 1...\n",
      "Processed chunk 1 with tags: ['Inclusive_Growth', 'Social_Sector_Development', 'Healthcare_Innovation']\n",
      "Saved: chapter_11_chunk_1.json\n",
      "Processing Chapter 11, Chunk 2...\n",
      "Processed chunk 2 with tags: ['Inclusive_Growth', 'Social_Sector_Expenditure', 'Economic_Development']\n",
      "Saved: chapter_11_chunk_2.json\n",
      "Processing Chapter 11, Chunk 3...\n",
      "Processed chunk 3 with tags: ['Education_Policy', 'School_Education', 'EdTech']\n",
      "Saved: chapter_11_chunk_3.json\n",
      "Processing Chapter 11, Chunk 4...\n",
      "Processed chunk 4 with tags: ['Healthcare_India', 'Economic_Survey', 'Public_Health']\n",
      "Saved: chapter_11_chunk_4.json\n",
      "Processing Chapter 11, Chunk 5...\n",
      "Processed chunk 5 with tags: ['Rural_Development', 'Sustainable_Livelihoods', 'Infrastructure_Development']\n",
      "Saved: chapter_11_chunk_5.json\n",
      "Processing Chapter 11, Chunk 6...\n",
      "Processed chunk 6 with tags: ['Welfare_Economics', 'Human_Development', 'Regulatory_Reforms']\n",
      "Saved: chapter_11_chunk_6.json\n",
      "Generating summary for Chapter 12...\n",
      "Generated summary for Chapter 12 (4938 chars)\n",
      "Processing Chapter 12, Chunk 1...\n",
      "Processed chunk 1 with tags: ['Labour_Market', 'Skill_Development', 'Job_Growth']\n",
      "Saved: chapter_12_chunk_1.json\n",
      "Processing Chapter 12, Chunk 2...\n",
      "Processed chunk 2 with tags: ['Demographic_Dividend', 'Employment_Growth', 'Economic_Development']\n",
      "Saved: chapter_12_chunk_2.json\n",
      "Processing Chapter 12, Chunk 3...\n",
      "Processed chunk 3 with tags: ['Employment_Growth', 'Female_Labour_Force_Participation', 'Wage_Trends']\n",
      "Saved: chapter_12_chunk_3.json\n",
      "Processing Chapter 12, Chunk 4...\n",
      "Processed chunk 4 with tags: ['Labour_Market_Regulations', 'Job_Creation_Initiatives', 'Renewable_Energy_Sector']\n",
      "Saved: chapter_12_chunk_4.json\n",
      "Processing Chapter 12, Chunk 5...\n",
      "Processed chunk 5 with tags: ['Skill Development', 'Employment Ecosystem', 'Upskilling Initiatives']\n",
      "Saved: chapter_12_chunk_5.json\n",
      "Processing Chapter 12, Chunk 6...\n",
      "Processed chunk 6 with tags: ['Labour_Reforms', 'Gender_Inclusivity', 'Economic_Growth']\n",
      "Saved: chapter_12_chunk_6.json\n",
      "Generating summary for Chapter 13...\n",
      "Generated summary for Chapter 13 (3003 chars)\n",
      "Processing Chapter 13, Chunk 1...\n",
      "Processed chunk 1 with tags: ['AI_and_Labor_Markets', 'Economic_Transformation', 'Inclusive_Growth']\n",
      "Saved: chapter_13_chunk_1.json\n",
      "Processing Chapter 13, Chunk 2...\n",
      "Processed chunk 2 with tags: ['AI_and_Labor_Markets', 'Technological_Unemployment', 'Economic_Impact_of_AI']\n",
      "Saved: chapter_13_chunk_2.json\n",
      "Processing Chapter 13, Chunk 3...\n",
      "Processed chunk 3 with tags: ['AI_Automation', 'Labour_Market_Disruption', 'Economic_Growth_Impact']\n",
      "Saved: chapter_13_chunk_3.json\n",
      "Processing Chapter 13, Chunk 4...\n",
      "Processed chunk 4 with tags: ['Institutional_Capacity', 'Economic_Resilience', 'Technological_Governance']\n",
      "Saved: chapter_13_chunk_4.json\n",
      "Processing Chapter 13, Chunk 5...\n",
      "Processed chunk 5 with tags: ['AI_Adoption_Challenges', 'Technological_Practicality', 'Resource_Efficiency']\n",
      "Saved: chapter_13_chunk_5.json\n",
      "Processing Chapter 13, Chunk 6...\n",
      "Processed chunk 6 with tags: ['AI_Adoption', 'Economic_Growth', 'Workforce_Development']\n",
      "Saved: chapter_13_chunk_6.json\n",
      "Processing Chapter 13, Chunk 7...\n",
      "Processed chunk 7 with tags: ['Labour_Market_Evolution', 'Technology_and_Employment', 'AI_Augmentation']\n",
      "Saved: chapter_13_chunk_7.json\n",
      "Processing Chapter 13, Chunk 8...\n",
      "Processed chunk 8 with tags: ['Services_Sector', 'AI_Adoption', 'Employment_Elasticity']\n",
      "Saved: chapter_13_chunk_8.json\n",
      "Processing Chapter 13, Chunk 9...\n",
      "Processed chunk 9 with tags: ['AI_Adoption', 'Labour_Market_Impacts', 'Economic_Resilience']\n",
      "Saved: chapter_13_chunk_9.json\n",
      "\n",
      "Document processing completed!\n",
      "Processed 94 chunks from 13 chapters\n",
      "Output saved to: /workspace/output/03/ES_24-25\n",
      "Processing summary saved to: /workspace/output/03/ES_24-25/processing_summary.json\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    base_path = \"/workspace/output/ES_24-25\"\n",
    "    output_directory = \"/workspace/output/03/ES_24-25\"\n",
    "\n",
    "    # Process all documents\n",
    "    result = process_documents(base_path, output_directory)\n",
    "\n",
    "    # Generate processing summary\n",
    "    generate_processing_summary(result, output_directory)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tata",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
