{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80cd9c84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "from pathlib import Path\n",
    "from typing import Dict, List\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from langchain_groq import ChatGroq\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c0a17d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize LLM\n",
    "llm = ChatGroq(\n",
    "    model=\"meta-llama/llama-4-scout-17b-16e-instruct\",\n",
    "    temperature=0.1,\n",
    ")\n",
    "\n",
    "\n",
    "# Pydantic Models\n",
    "class ChapterSummary(BaseModel):\n",
    "    summary: str = Field(\n",
    "        ...,\n",
    "        description=\"Comprehensive summary of the chapter retaining all facts and figures\",\n",
    "    )\n",
    "\n",
    "\n",
    "class SubchapterTags(BaseModel):\n",
    "    tags: List[str] = Field(\n",
    "        ..., description=\"List of 1-3 relevant tags for knowledge graph\", max_length=3\n",
    "    )\n",
    "\n",
    "\n",
    "class ChunkOutput(BaseModel):\n",
    "    chapter_no: int = Field(..., description=\"Chapter number\")\n",
    "    subchapter_no: int = Field(..., description=\"Subchapter/chunk number\")\n",
    "    content: str = Field(..., description=\"Chapter summary + subchapter content\")\n",
    "    tags: List[str] = Field(..., description=\"Generated tags for the subchapter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b18799",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file_content(file_path: str) -> str:\n",
    "    \"\"\"Read content from a markdown file\"\"\"\n",
    "    try:\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            return file.read()\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading file {file_path}: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "def extract_chapter_number(filename: str) -> int:\n",
    "    \"\"\"Extract chapter number from filename\"\"\"\n",
    "    match = re.search(r\"chapter_(\\d+)\", filename)\n",
    "    return int(match.group(1)) if match else 0\n",
    "\n",
    "\n",
    "def extract_chunk_number(filename: str) -> int:\n",
    "    \"\"\"Extract chunk number from filename\"\"\"\n",
    "    match = re.search(r\"chunk_(\\d+)\", filename)\n",
    "    return int(match.group(1)) if match else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "368fafa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_chapter_summary(chapter_content: str) -> str:\n",
    "    \"\"\"Generate summary with error handling for 413, 400, 429 using recursion.\"\"\"\n",
    "\n",
    "    structured_llm = llm.with_structured_output(ChapterSummary)\n",
    "\n",
    "    prompt = f\"\"\"You are tasked with creating a comprehensive summary of an economic survey chapter.\n",
    "\n",
    "    Instructions:\n",
    "    - Retain ALL numerical data, statistics, percentages, and figures mentioned\n",
    "    - Include key policy recommendations and findings  \n",
    "    - Maintain the factual accuracy of economic indicators\n",
    "    - Keep the summary detailed enough to understand the chapter's main points\n",
    "    - Focus on economic trends, challenges, and outlook presented\n",
    "\n",
    "    Chapter Content:\n",
    "    {chapter_content}\n",
    "\n",
    "    Generate a detailed summary that preserves all important facts and figures.\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        result = structured_llm.invoke(prompt)\n",
    "        return result.summary\n",
    "\n",
    "    except Exception as e:\n",
    "        # Parse Groq error JSON if possible\n",
    "        try:\n",
    "            err_obj = json.loads(str(e).split(\"Error code:\", 1)[-1].strip())\n",
    "        except Exception:\n",
    "            err_obj = {\"error\": {\"message\": str(e)}}\n",
    "\n",
    "        error_code = err_obj.get(\"error\", {}).get(\"code\", \"\")\n",
    "        error_msg = err_obj.get(\"error\", {}).get(\"message\", \"\")\n",
    "\n",
    "        MAX_CHARS_SAFE = 30000\n",
    "\n",
    "        # --- Handle 413: Request too large ---\n",
    "        if \"413\" in str(e) or \"Request too large\" in error_msg:\n",
    "            print(\"⚠️ 413 error: input too large. Trimming content and retrying...\")\n",
    "            trimmed_content = chapter_content[:MAX_CHARS_SAFE]\n",
    "            return generate_chapter_summary(trimmed_content)\n",
    "\n",
    "        # --- Handle 400: tool_use_failed, return failed_generation ---\n",
    "        if \"400\" in str(e) or \"tool_use_failed\" in error_code:\n",
    "            failed_text = err_obj.get(\"error\", {}).get(\"failed_generation\")\n",
    "            if failed_text:\n",
    "                print(\"⚠️ 400 error: returning failed_generation output.\")\n",
    "                return failed_text\n",
    "            return f\"Summary generation failed with 400: {error_msg}\"\n",
    "\n",
    "        # --- Handle 429: Rate limit exceeded ---\n",
    "        if \"429\" in str(e) or \"rate_limit\" in error_code:\n",
    "            print(\"⚠️ 429 error: rate limit exceeded. Waiting 60s before retry...\")\n",
    "            time.sleep(60)\n",
    "            return generate_chapter_summary(chapter_content)\n",
    "\n",
    "        # --- Other errors ---\n",
    "        print(f\"❌ Unhandled error: {error_msg}\")\n",
    "        return f\"Summary generation failed: {error_msg}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d196cc9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_subchapter_tags(subchapter_content: str) -> List[str]:\n",
    "    \"\"\"Generate tags for a subchapter\"\"\"\n",
    "    structured_llm = llm.with_structured_output(SubchapterTags)\n",
    "\n",
    "    prompt = f\"\"\"Analyze the provided economic survey subchapter content and generate 1-3 relevant tags.\n",
    "\n",
    "    Instructions:\n",
    "    - Tags should be suitable for a knowledge graph\n",
    "    - Focus on main economic themes, sectors, or concepts\n",
    "    - Use concise, descriptive terms (e.g., \"GDP_Growth\", \"Inflation\", \"Manufacturing\", \"Trade_Policy\")\n",
    "    - Avoid generic tags like \"economics\" or \"data\"\n",
    "    - Prioritize the most specific and relevant topics\n",
    "\n",
    "    Subchapter Content:\n",
    "    {subchapter_content}\n",
    "\n",
    "    Generate up to 3 specific tags that best represent this content for knowledge graph categorization.\"\"\"\n",
    "\n",
    "    try:\n",
    "        result = structured_llm.invoke(prompt)\n",
    "        return result.tags\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating tags: {e}\")\n",
    "        return [\"processing_error\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8621659e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def discover_chapters_and_chunks(base_path: str) -> Dict:\n",
    "    \"\"\"Discover all chapters and their chunks\"\"\"\n",
    "    base_path = Path(base_path)\n",
    "    chapters_data = {}\n",
    "\n",
    "    # Find all chapter directories\n",
    "    chapter_dirs = [\n",
    "        d for d in base_path.iterdir() if d.is_dir() and d.name.startswith(\"chapter_\")\n",
    "    ]\n",
    "    chapter_dirs.sort(key=lambda x: extract_chapter_number(x.name))\n",
    "\n",
    "    for chapter_dir in chapter_dirs:\n",
    "        chapter_no = extract_chapter_number(chapter_dir.name)\n",
    "\n",
    "        # Find main chapter file\n",
    "        chapter_file = chapter_dir / f\"chapter_{chapter_no}.md\"\n",
    "\n",
    "        # Find chunk files in this chapter\n",
    "        chunk_files = [\n",
    "            f\n",
    "            for f in chapter_dir.iterdir()\n",
    "            if f.is_file() and f.name.startswith(\"chunk_\") and f.name.endswith(\".md\")\n",
    "        ]\n",
    "        chunk_files.sort(key=lambda x: extract_chunk_number(x.name))\n",
    "\n",
    "        chapters_data[chapter_no] = {\n",
    "            \"chapter_file\": str(chapter_file) if chapter_file.exists() else None,\n",
    "            \"chunk_files\": [\n",
    "                (extract_chunk_number(f.name), str(f)) for f in chunk_files\n",
    "            ],\n",
    "        }\n",
    "\n",
    "    total_chunks = sum(len(data[\"chunk_files\"]) for data in chapters_data.values())\n",
    "    print(f\"Discovered {total_chunks} chunks across {len(chapters_data)} chapters\")\n",
    "\n",
    "    return chapters_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "531b1704",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_chapter_summary(chapter_file_path: str, chapter_no: int) -> str:\n",
    "    \"\"\"Process and generate summary for a chapter\"\"\"\n",
    "    if not chapter_file_path or not os.path.exists(chapter_file_path):\n",
    "        print(f\"Warning: Chapter {chapter_no} file not found at {chapter_file_path}\")\n",
    "        return f\"Chapter {chapter_no} Summary (file not found)\"\n",
    "\n",
    "    print(f\"Generating summary for Chapter {chapter_no}...\")\n",
    "    chapter_content = read_file_content(chapter_file_path)\n",
    "\n",
    "    if chapter_content:\n",
    "        chapter_summary = generate_chapter_summary(chapter_content)\n",
    "        print(\n",
    "            f\"Generated summary for Chapter {chapter_no} ({len(chapter_summary)} chars)\"\n",
    "        )\n",
    "        return chapter_summary\n",
    "    else:\n",
    "        return f\"Chapter {chapter_no} Summary (content not readable)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee2caa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_chunk(\n",
    "    chunk_no: int, chunk_file_path: str, chapter_no: int, chapter_summary: str\n",
    ") -> Dict:\n",
    "    \"\"\"Process a single chunk\"\"\"\n",
    "    print(f\"Processing Chapter {chapter_no}, Chunk {chunk_no}...\")\n",
    "\n",
    "    # Read chunk content\n",
    "    chunk_content = read_file_content(chunk_file_path)\n",
    "\n",
    "    if not chunk_content:\n",
    "        print(f\"Warning: Could not read chunk file {chunk_file_path}\")\n",
    "        return None\n",
    "\n",
    "    # Generate tags for this subchapter\n",
    "    tags = generate_subchapter_tags(chunk_content)\n",
    "\n",
    "    # Combine chapter summary with chunk content\n",
    "    combined_content = (\n",
    "        f\"Chapter {chapter_no} Summary:\\n{chapter_summary}\\n\\n\"\n",
    "        f\"Subchapter {chunk_no} Content:\\n{chunk_content}\"\n",
    "    )\n",
    "\n",
    "    # Create chunk output\n",
    "    chunk_output = {\n",
    "        \"chapter_no\": chapter_no,\n",
    "        \"subchapter_no\": chunk_no,\n",
    "        \"content\": combined_content,\n",
    "        \"tags\": tags,\n",
    "    }\n",
    "\n",
    "    print(f\"Processed chunk {chunk_no} with tags: {tags}\")\n",
    "    return chunk_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d87826",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_chunk_as_json(chunk_data: Dict, output_dir: str) -> None:\n",
    "    \"\"\"Save a single chunk as JSON file\"\"\"\n",
    "    output_dir = Path(output_dir)\n",
    "    output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "    filename = (\n",
    "        f\"chapter_{chunk_data['chapter_no']}_chunk_{chunk_data['subchapter_no']}.json\"\n",
    "    )\n",
    "    file_path = output_dir / filename\n",
    "\n",
    "    try:\n",
    "        with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(chunk_data, f, indent=2, ensure_ascii=False)\n",
    "        print(f\"Saved: {filename}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving {filename}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb0af75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_documents(base_path: str, output_dir: str = \"processed_chunks\") -> Dict:\n",
    "    \"\"\"Main function to process all documents\"\"\"\n",
    "    print(\"Starting document processing...\")\n",
    "\n",
    "    # Discover all chapters and chunks\n",
    "    chapters_data = discover_chapters_and_chunks(base_path)\n",
    "\n",
    "    if not chapters_data:\n",
    "        print(\"No chapters found in the specified path\")\n",
    "        return {}\n",
    "\n",
    "    # Process each chapter\n",
    "    chapter_summaries = {}\n",
    "    processed_chunks = []\n",
    "\n",
    "    for chapter_no in sorted(chapters_data.keys()):\n",
    "        chapter_info = chapters_data[chapter_no]\n",
    "\n",
    "        # Generate chapter summary if not already done\n",
    "        if chapter_no not in chapter_summaries:\n",
    "            chapter_summaries[chapter_no] = process_chapter_summary(\n",
    "                chapter_info[\"chapter_file\"], chapter_no\n",
    "            )\n",
    "\n",
    "        # Process each chunk in this chapter\n",
    "        for chunk_no, chunk_file_path in chapter_info[\"chunk_files\"]:\n",
    "            chunk_data = process_chunk(\n",
    "                chunk_no, chunk_file_path, chapter_no, chapter_summaries[chapter_no]\n",
    "            )\n",
    "\n",
    "            if chunk_data:\n",
    "                # Save immediately to avoid memory issues with large datasets\n",
    "                save_chunk_as_json(chunk_data, output_dir)\n",
    "                processed_chunks.append(\n",
    "                    {\"chapter_no\": chapter_no, \"chunk_no\": chunk_no, \"file_saved\": True}\n",
    "                )\n",
    "\n",
    "    print(\"\\nDocument processing completed!\")\n",
    "    print(\n",
    "        f\"Processed {len(processed_chunks)} chunks from {len(chapters_data)} chapters\"\n",
    "    )\n",
    "    print(f\"Output saved to: {output_dir}\")\n",
    "\n",
    "    return {\n",
    "        \"total_chapters\": len(chapters_data),\n",
    "        \"total_chunks\": len(processed_chunks),\n",
    "        \"output_directory\": output_dir,\n",
    "        \"processed_chunks\": processed_chunks,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e77f6e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_processing_summary(result: Dict, output_dir: str) -> None:\n",
    "    \"\"\"Generate a summary of processing results\"\"\"\n",
    "    summary_file = Path(output_dir) / \"processing_summary.json\"\n",
    "\n",
    "    try:\n",
    "        with open(summary_file, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(result, f, indent=2, ensure_ascii=False)\n",
    "        print(f\"Processing summary saved to: {summary_file}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving processing summary: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99dc10d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if __name__ == \"__main__\":\n",
    "#     base_path = \"/workspace/output/02\"\n",
    "#     output_directory = \"/workspace/output/03/\"\n",
    "\n",
    "#     # Process all documents\n",
    "#     files = [\n",
    "#         f for f in os.listdir(base_path) if os.path.isdir(os.path.join(base_path, f))\n",
    "#     ]\n",
    "\n",
    "#     for i in files:\n",
    "#         print(f\"Processing: {i}\")\n",
    "#         result = process_documents(\n",
    "#             os.path.join(base_path, i), os.path.join(output_directory, i)\n",
    "#         )\n",
    "#         generate_processing_summary(result, os.path.join(output_directory, i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e0c2fed8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting document processing...\n",
      "Discovered 84 chunks across 13 chapters\n",
      "Generating summary for Chapter 1...\n",
      "Generated summary for Chapter 1 (2041 chars)\n",
      "Processing Chapter 1, Chunk 1...\n",
      "Processed chunk 1 with tags: ['GDP_Growth', 'Economic_Recovery', 'Public_Spending']\n",
      "Saved: chapter_1_chunk_1.json\n",
      "Processing Chapter 1, Chunk 2...\n",
      "Processed chunk 2 with tags: ['Global_Economic_Growth', 'Inflation_Pressures', 'Geopolitical_Risks']\n",
      "Saved: chapter_1_chunk_2.json\n",
      "Processing Chapter 1, Chunk 3...\n",
      "Processed chunk 3 with tags: ['GDP_Growth', 'Manufacturing', 'Investment_Demand']\n",
      "Saved: chapter_1_chunk_3.json\n",
      "Processing Chapter 1, Chunk 4...\n",
      "Processed chunk 4 with tags: ['Fiscal_Consolidation', 'Macroeconomic_Stability', 'Economic_Growth']\n",
      "Saved: chapter_1_chunk_4.json\n",
      "Processing Chapter 1, Chunk 5...\n",
      "Processed chunk 5 with tags: ['Inclusive_Growth', 'Poverty_Reduction', 'Social_Welfare']\n",
      "Saved: chapter_1_chunk_5.json\n",
      "Processing Chapter 1, Chunk 6...\n",
      "Processed chunk 6 with tags: ['GDP_Growth', 'Monetary_Policy', 'Global_Economy']\n",
      "Saved: chapter_1_chunk_6.json\n",
      "Generating summary for Chapter 2...\n",
      "⚠️ 413 error: input too large. Trimming content and retrying...\n",
      "Generated summary for Chapter 2 (2663 chars)\n",
      "Processing Chapter 2, Chunk 1...\n",
      "Processed chunk 1 with tags: ['Financial_Stability', 'Monetary_Management', 'Capital_Market_Growth']\n",
      "Saved: chapter_2_chunk_1.json\n",
      "Processing Chapter 2, Chunk 2...\n",
      "Processed chunk 2 with tags: ['Financial_Sector_Performance', 'Monetary_Policy', 'Financial_Inclusion']\n",
      "Saved: chapter_2_chunk_2.json\n",
      "Processing Chapter 2, Chunk 3...\n",
      "Processed chunk 3 with tags: ['Monetary_Policy', 'Liquidity_Conditions', 'Banking_System']\n",
      "Saved: chapter_2_chunk_3.json\n",
      "Processing Chapter 2, Chunk 4...\n",
      "Processed chunk 4 with tags: ['Financial_Intermediation', 'Economic_Growth', 'Financial_Regulation']\n",
      "Saved: chapter_2_chunk_4.json\n",
      "Processing Chapter 2, Chunk 5...\n",
      "Processed chunk 5 with tags: ['BankingSectorPerformance', 'CreditAvailability', 'FinancialInclusion']\n",
      "Saved: chapter_2_chunk_5.json\n",
      "Processing Chapter 2, Chunk 6...\n",
      "Processed chunk 6 with tags: ['Indian_Capital_Markets', 'Financial_Development', 'Retail_Investor_Participation']\n",
      "Saved: chapter_2_chunk_6.json\n",
      "Processing Chapter 2, Chunk 7...\n",
      "Processed chunk 7 with tags: ['Insurance_Sector', 'Economic_Growth', 'Regulatory_Support']\n",
      "Saved: chapter_2_chunk_7.json\n",
      "Processing Chapter 2, Chunk 8...\n",
      "Processed chunk 8 with tags: ['Pension_Sector', 'Financial_Stability', 'Regulatory_Frameworks']\n",
      "Saved: chapter_2_chunk_8.json\n",
      "Processing Chapter 2, Chunk 9...\n",
      "Processed chunk 9 with tags: ['Financial_Inclusion', 'Digital_Payments', 'Financial_Development']\n",
      "Saved: chapter_2_chunk_9.json\n",
      "Generating summary for Chapter 3...\n",
      "Generated summary for Chapter 3 (1655 chars)\n",
      "Processing Chapter 3, Chunk 1...\n",
      "Processed chunk 1 with tags: ['Inflation_Control', 'Monetary_Policy', 'Food_Prices']\n",
      "Saved: chapter_3_chunk_1.json\n",
      "Processing Chapter 3, Chunk 2...\n",
      "Processed chunk 2 with tags: ['Inflation_Management', 'Monetary_Policy', 'Economic_Resilience']\n",
      "Saved: chapter_3_chunk_2.json\n",
      "Processing Chapter 3, Chunk 3...\n",
      "Processed chunk 3 with tags: ['Retail_Inflation', 'Monetary_Policy', 'Fuel_Prices']\n",
      "Saved: chapter_3_chunk_3.json\n",
      "Processing Chapter 3, Chunk 4...\n",
      "Processed chunk 4 with tags: ['Core_Inflation', 'Post_Pandemic_Economy', 'Monetary_Policy']\n",
      "Saved: chapter_3_chunk_4.json\n",
      "Processing Chapter 3, Chunk 5...\n",
      "Processed chunk 5 with tags: ['Food_Inflation', 'Climate_Change', 'Agricultural_Economics']\n",
      "Saved: chapter_3_chunk_5.json\n",
      "Processing Chapter 3, Chunk 6...\n",
      "Processed chunk 6 with tags: ['Retail_Inflation', 'Interstate_Variations', 'Rural_Urban_Inflation']\n",
      "Saved: chapter_3_chunk_6.json\n",
      "Processing Chapter 3, Chunk 7...\n",
      "Processed chunk 7 with tags: ['Inflation_Outlook', 'Commodity_Prices', 'Food_Price_Stability']\n",
      "Saved: chapter_3_chunk_7.json\n",
      "Generating summary for Chapter 4...\n",
      "⚠️ 413 error: input too large. Trimming content and retrying...\n",
      "Generated summary for Chapter 4 (3201 chars)\n",
      "Processing Chapter 4, Chunk 1...\n",
      "Processed chunk 1 with tags: ['Trade_Balance', 'External_Sector', 'Global_Value_Chains']\n",
      "Saved: chapter_4_chunk_1.json\n",
      "Processing Chapter 4, Chunk 2...\n",
      "Processed chunk 2 with tags: ['Global_Trade_Dynamics', 'Foreign_Investment_Trends', 'External_Sector_Performance']\n",
      "Saved: chapter_4_chunk_2.json\n",
      "Processing Chapter 4, Chunk 3...\n",
      "Processed chunk 3 with tags: ['Global_Trade_Dynamics', 'Trade_Reconfiguration', 'Deglobalization']\n",
      "Saved: chapter_4_chunk_3.json\n",
      "Processing Chapter 4, Chunk 4...\n",
      "Processed chunk 4 with tags: ['India Trade Policy', 'Global Value Chains', 'Trade Facilitation']\n",
      "Saved: chapter_4_chunk_4.json\n",
      "Processing Chapter 4, Chunk 5...\n",
      "Processed chunk 5 with tags: ['Remittances', 'Current_Account_Balance', 'Trade_Policy']\n",
      "Saved: chapter_4_chunk_5.json\n",
      "Processing Chapter 4, Chunk 6...\n",
      "Processed chunk 6 with tags: ['FDI_Inflows', 'Capital_Account_Balance', 'Exchange_Rate_Dynamics']\n",
      "Saved: chapter_4_chunk_6.json\n",
      "Processing Chapter 4, Chunk 7...\n",
      "Processed chunk 7 with tags: ['External_Debt', 'Debt_Sustainability', 'Current_Account_Deficit']\n",
      "Saved: chapter_4_chunk_7.json\n",
      "Processing Chapter 4, Chunk 8...\n",
      "Processed chunk 8 with tags: ['Trade_Policy', 'Manufacturing', 'Globalisation']\n",
      "Saved: chapter_4_chunk_8.json\n",
      "Generating summary for Chapter 5...\n",
      "Generated summary for Chapter 5 (8583 chars)\n",
      "Processing Chapter 5, Chunk 1...\n",
      "Processed chunk 1 with tags: ['India_Economic_Growth', 'Medium_Term_Outlook', 'Sustainable_Development']\n",
      "Saved: chapter_5_chunk_1.json\n",
      "Processing Chapter 5, Chunk 2...\n",
      "Processed chunk 2 with tags: ['Economic_Growth', 'Sustainable_Development', 'Globalization_Impact']\n",
      "Saved: chapter_5_chunk_2.json\n",
      "Processing Chapter 5, Chunk 3...\n",
      "Processed chunk 3 with tags: ['Economic Growth Strategy', 'Sustainable Development', 'India Economic Policy']\n",
      "Saved: chapter_5_chunk_3.json\n",
      "Processing Chapter 5, Chunk 4...\n",
      "Processed chunk 4 with tags: ['India_Economic_Growth', 'Sustainable_Development', 'Medium_Term_Outlook']\n",
      "Saved: chapter_5_chunk_4.json\n",
      "Generating summary for Chapter 6...\n",
      "Generated summary for Chapter 6 (5542 chars)\n",
      "Processing Chapter 6, Chunk 1...\n",
      "Processed chunk 1 with tags: ['Climate_Change', 'Energy_Transition', 'Sustainable_Development']\n",
      "Saved: chapter_6_chunk_1.json\n",
      "Processing Chapter 6, Chunk 2...\n",
      "Processed chunk 2 with tags: ['Climate_Action', 'Renewable_Energy', 'Sustainable_Development']\n",
      "Saved: chapter_6_chunk_2.json\n",
      "Processing Chapter 6, Chunk 3...\n",
      "Processed chunk 3 with tags: ['Climate_Change_Adaptation', 'Sustainable_Energy', 'Low_Carbon_Development']\n",
      "Saved: chapter_6_chunk_3.json\n",
      "Processing Chapter 6, Chunk 4...\n",
      "Processed chunk 4 with tags: ['Climate_Finance', 'Sustainable_Development', 'Green_Economy']\n",
      "Saved: chapter_6_chunk_4.json\n",
      "Processing Chapter 6, Chunk 5...\n",
      "Processed chunk 5 with tags: ['Climate_Change_Mitigation', 'Renewable_Energy', 'Sustainable_Development']\n",
      "Saved: chapter_6_chunk_5.json\n",
      "Processing Chapter 6, Chunk 6...\n",
      "Processed chunk 6 with tags: ['Renewable_Energy', 'Energy_Security', 'Low_Carbon_Economy']\n",
      "Saved: chapter_6_chunk_6.json\n",
      "Generating summary for Chapter 7...\n",
      "⚠️ 413 error: input too large. Trimming content and retrying...\n",
      "Generated summary for Chapter 7 (1108 chars)\n",
      "Processing Chapter 7, Chunk 1...\n",
      "Processed chunk 1 with tags: ['Social_Empowerment', 'Economic_Growth', 'Welfare_Programs']\n",
      "Saved: chapter_7_chunk_1.json\n",
      "Processing Chapter 7, Chunk 2...\n",
      "Processed chunk 2 with tags: ['Economic_Growth', 'Social_Sector_Expenditure', 'Human_Development']\n",
      "Saved: chapter_7_chunk_2.json\n",
      "Processing Chapter 7, Chunk 3...\n",
      "Processed chunk 3 with tags: ['Welfare Policy', 'Economic Growth', 'Sustainable Development']\n",
      "Saved: chapter_7_chunk_3.json\n",
      "Processing Chapter 7, Chunk 4...\n",
      "Processed chunk 4 with tags: ['WomenEmpowerment', 'EconomicDevelopment', 'GenderEquality']\n",
      "Saved: chapter_7_chunk_4.json\n",
      "Processing Chapter 7, Chunk 5...\n",
      "Processed chunk 5 with tags: ['Rural_Economy', 'Sustainable_Development', 'Financial_Inclusion']\n",
      "Saved: chapter_7_chunk_5.json\n",
      "Processing Chapter 7, Chunk 6...\n",
      "Processed chunk 6 with tags: ['Sustainable_Development_Goals', 'SDG_Progress', 'Economic_Resilience']\n",
      "Saved: chapter_7_chunk_6.json\n",
      "Processing Chapter 7, Chunk 7...\n",
      "Processed chunk 7 with tags: ['Welfare_Economics', 'Human_Development', 'Economic_Empowerment']\n",
      "Saved: chapter_7_chunk_7.json\n",
      "Generating summary for Chapter 8...\n",
      "⚠️ 413 error: input too large. Trimming content and retrying...\n",
      "Generated summary for Chapter 8 (2512 chars)\n",
      "Processing Chapter 8, Chunk 1...\n",
      "Processed chunk 1 with tags: ['Employment_Growth', 'Skill_Development', 'Labour_Market']\n",
      "Saved: chapter_8_chunk_1.json\n",
      "Processing Chapter 8, Chunk 2...\n",
      "Processed chunk 2 with tags: ['Employment_Growth', 'Skill_Development', 'Labour_Market_Trends']\n",
      "Saved: chapter_8_chunk_2.json\n",
      "Processing Chapter 8, Chunk 3...\n",
      "Processed chunk 3 with tags: ['Employment Trends', 'Labour Market Indicators', 'Economic Growth']\n",
      "Saved: chapter_8_chunk_3.json\n",
      "Processing Chapter 8, Chunk 4...\n",
      "Processed chunk 4 with tags: ['Youth_Employment', 'Female_LFPR', 'Manufacturing_Growth']\n",
      "Saved: chapter_8_chunk_4.json\n",
      "Processing Chapter 8, Chunk 5...\n",
      "Processed chunk 5 with tags: ['Gig_Economy', 'AI_Automation', 'Green_Energy_Transition']\n",
      "Saved: chapter_8_chunk_5.json\n",
      "Processing Chapter 8, Chunk 6...\n",
      "Processed chunk 6 with tags: ['Job_Creation', 'Agro_Processing', 'Care_Economy']\n",
      "Saved: chapter_8_chunk_6.json\n",
      "Processing Chapter 8, Chunk 7...\n",
      "Processed chunk 7 with tags: ['Skill_Development', 'Employment_Training', 'Apprenticeship_Programs']\n",
      "Saved: chapter_8_chunk_7.json\n",
      "Processing Chapter 8, Chunk 8...\n",
      "Processed chunk 8 with tags: ['Employment_Growth', 'Skill_Development', 'Economic_Resilience']\n",
      "Saved: chapter_8_chunk_8.json\n",
      "Generating summary for Chapter 9...\n",
      "Generated summary for Chapter 9 (5567 chars)\n",
      "Processing Chapter 9, Chunk 1...\n",
      "Processed chunk 1 with tags: ['Agriculture', 'Food_Security', 'Sustainable_Development']\n",
      "Saved: chapter_9_chunk_1.json\n",
      "Processing Chapter 9, Chunk 2...\n",
      "Processed chunk 2 with tags: ['Agriculture_Productivity', 'Sustainable_Farming', 'Livestock_Development']\n",
      "Saved: chapter_9_chunk_2.json\n",
      "Processing Chapter 9, Chunk 3...\n",
      "Processed chunk 3 with tags: ['CropDiversification', 'AgriculturalInvestment', 'SustainableAgriculture']\n",
      "Saved: chapter_9_chunk_3.json\n",
      "Processing Chapter 9, Chunk 4...\n",
      "Processed chunk 4 with tags: ['Animal_Husbandry', 'Dairying_and_Fisheries', 'Agricultural_Growth']\n",
      "Saved: chapter_9_chunk_4.json\n",
      "Processing Chapter 9, Chunk 5...\n",
      "Processed chunk 5 with tags: ['Food_Processing_Industry', 'Agricultural_Exports', 'Food_Security']\n",
      "Saved: chapter_9_chunk_5.json\n",
      "Processing Chapter 9, Chunk 6...\n",
      "Processed chunk 6 with tags: ['Agriculture_Sector', 'Sustainable_Agriculture', 'Farm_Income_Enhancement']\n",
      "Saved: chapter_9_chunk_6.json\n",
      "Generating summary for Chapter 10...\n",
      "Generated summary for Chapter 10 (4816 chars)\n",
      "Processing Chapter 10, Chunk 1...\n",
      "Processed chunk 1 with tags: ['Manufacturing_Growth', 'Industrial_Development', 'Economic_Expansion']\n",
      "Saved: chapter_10_chunk_1.json\n",
      "Processing Chapter 10, Chunk 2...\n",
      "Processed chunk 2 with tags: ['Cement_Industry', 'Steel_Sector', 'Coal_Production']\n",
      "Saved: chapter_10_chunk_2.json\n",
      "Processing Chapter 10, Chunk 3...\n",
      "Processed chunk 3 with tags: ['Manufacturing_Incentives', 'MSME_Sector', 'Industrial_Development']\n",
      "Saved: chapter_10_chunk_3.json\n",
      "Processing Chapter 10, Chunk 4...\n",
      "Processed chunk 4 with tags: ['Industrial_Development', 'Manufacturing_Sectors', 'Economic_Outlook']\n",
      "Saved: chapter_10_chunk_4.json\n",
      "Generating summary for Chapter 11...\n",
      "Generated summary for Chapter 11 (4920 chars)\n",
      "Processing Chapter 11, Chunk 1...\n",
      "Processed chunk 1 with tags: ['Digital_Services', 'Services_Sector_Growth', 'High_Tech_Skills']\n",
      "Saved: chapter_11_chunk_1.json\n",
      "Processing Chapter 11, Chunk 2...\n",
      "Processed chunk 2 with tags: ['Services_Sector', 'Economic_Growth', 'India_Economy']\n",
      "Saved: chapter_11_chunk_2.json\n",
      "Processing Chapter 11, Chunk 3...\n",
      "Processed chunk 3 with tags: ['Services_Sector_Growth', 'Digital_Services_Exports', 'India_Economic_Performance']\n",
      "Saved: chapter_11_chunk_3.json\n",
      "Processing Chapter 11, Chunk 4...\n",
      "Processed chunk 4 with tags: ['Services_Sector_Financing', 'Bank_Credit_Growth', 'External_Financing_Trends']\n",
      "Saved: chapter_11_chunk_4.json\n",
      "Processing Chapter 11, Chunk 5...\n",
      "Processed chunk 5 with tags: ['TransportationInfrastructure', 'DigitalEconomy', 'RealEstateDevelopment']\n",
      "Saved: chapter_11_chunk_5.json\n",
      "Processing Chapter 11, Chunk 6...\n",
      "Processed chunk 6 with tags: ['Digital_Skills_Gap', 'Logistics_Development', 'MSME_Financing']\n",
      "Saved: chapter_11_chunk_6.json\n",
      "Processing Chapter 11, Chunk 7...\n",
      "Processed chunk 7 with tags: ['Services_Sector_Transformation', 'Digital_Economy', 'Skilling_and_Job_Market']\n",
      "Saved: chapter_11_chunk_7.json\n",
      "Generating summary for Chapter 12...\n",
      "Generated summary for Chapter 12 (6958 chars)\n",
      "Processing Chapter 12, Chunk 1...\n",
      "Processed chunk 1 with tags: ['Infrastructure_Development', 'Public_Investment', 'Economic_Growth']\n",
      "Saved: chapter_12_chunk_1.json\n",
      "Processing Chapter 12, Chunk 2...\n",
      "Processed chunk 2 with tags: ['Infrastructure_Development', 'India_Economic_Strategy', 'ViksitBharat_2047']\n",
      "Saved: chapter_12_chunk_2.json\n",
      "Processing Chapter 12, Chunk 3...\n",
      "Processed chunk 3 with tags: ['Infrastructure_Financing', 'Public_Expenditure', 'India_Infrastructure_Development']\n",
      "Saved: chapter_12_chunk_3.json\n",
      "Processing Chapter 12, Chunk 4...\n",
      "Processed chunk 4 with tags: ['Infrastructure Development', 'Economic Growth', 'Sustainable Infrastructure']\n",
      "Saved: chapter_12_chunk_4.json\n",
      "Processing Chapter 12, Chunk 5...\n",
      "Processed chunk 5 with tags: ['Infrastructure_Financing', 'Private_Sector_Participation', 'Sustainable_Development']\n",
      "Saved: chapter_12_chunk_5.json\n",
      "Processing Chapter 12, Chunk 6...\n",
      "Processed chunk 6 with tags: ['Infrastructure_Development', 'Logistics_Efficiency', 'National_Investment_Pipeline']\n",
      "Saved: chapter_12_chunk_6.json\n",
      "Processing Chapter 12, Chunk 7...\n",
      "Processed chunk 7 with tags: ['Infrastructure_Development', 'Private_Sector_Financing', 'Data_Management']\n",
      "Saved: chapter_12_chunk_7.json\n",
      "Generating summary for Chapter 13...\n",
      "Generated summary for Chapter 13 (1114 chars)\n",
      "Processing Chapter 13, Chunk 1...\n",
      "Processed chunk 1 with tags: ['Sustainable_Development', 'Climate_Action', 'Economic_Growth']\n",
      "Saved: chapter_13_chunk_1.json\n",
      "Processing Chapter 13, Chunk 2...\n",
      "Processed chunk 2 with tags: ['Climate_Change', 'Sustainable_Development', 'Renewable_Energy']\n",
      "Saved: chapter_13_chunk_2.json\n",
      "Processing Chapter 13, Chunk 3...\n",
      "Processed chunk 3 with tags: ['Sustainable Development', 'Climate Change Strategy', 'Environmental Impact']\n",
      "Saved: chapter_13_chunk_3.json\n",
      "Processing Chapter 13, Chunk 4...\n",
      "Processed chunk 4 with tags: ['Sustainable_Lifestyle', 'Environmental_Resilience', 'Climate_Action']\n",
      "Saved: chapter_13_chunk_4.json\n",
      "Processing Chapter 13, Chunk 5...\n",
      "Processed chunk 5 with tags: ['Sustainable_Consumption', 'Equanimity', 'Climate_Change']\n",
      "Saved: chapter_13_chunk_5.json\n",
      "\n",
      "Document processing completed!\n",
      "Processed 84 chunks from 13 chapters\n",
      "Output saved to: /workspace/output/03/ES_23-24\n",
      "Processing summary saved to: /workspace/output/03/ES_23-24/processing_summary.json\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    base_path = \"/workspace/output/02/ES_23-24\"\n",
    "    output_directory = \"/workspace/output/03/ES_23-24\"\n",
    "\n",
    "    # Process all documents\n",
    "    result = process_documents(base_path, output_directory)\n",
    "\n",
    "    # Generate processing summary\n",
    "    generate_processing_summary(result, output_directory)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tata",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
