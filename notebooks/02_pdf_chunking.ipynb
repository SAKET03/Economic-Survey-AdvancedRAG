{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 1,
            "id": "imports",
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "True"
                        ]
                    },
                    "execution_count": 1,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "import json\n",
                "import os\n",
                "import re\n",
                "\n",
                "from docling.document_converter import DocumentConverter\n",
                "from dotenv import load_dotenv\n",
                "\n",
                "load_dotenv()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "id": "setup",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Initialize document converter\n",
                "converter = DocumentConverter()\n",
                "\n",
                "# Configuration\n",
                "BASE_PATH = \"/workspace/output/01\"  # Change this to your base folder path\n",
                "# OUTPUT_PATH = \"/workspace/output/02\"  # Output path for chunks\n",
                "OUTPUT_PATH = \"/workspace/output/\"  # Output path for chunks\n",
                "\n",
                "# Create output directory if it doesn't exist\n",
                "os.makedirs(OUTPUT_PATH, exist_ok=True)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "id": "utility_functions",
            "metadata": {},
            "outputs": [],
            "source": [
                "def load_chapter_structure(folder_path):\n",
                "    \"\"\"Load chapter structure from JSON file.\"\"\"\n",
                "    json_file_path = os.path.join(folder_path, \"chapter_structure.json\")\n",
                "    if os.path.exists(json_file_path):\n",
                "        with open(json_file_path, \"r\", encoding=\"utf-8\") as f:\n",
                "            chapter_structure = json.load(f)\n",
                "        return chapter_structure\n",
                "    else:\n",
                "        print(f\"Chapter structure file not found in {folder_path}\")\n",
                "        return None\n",
                "\n",
                "\n",
                "def convert_pdf_to_markdown(pdf_path):\n",
                "    \"\"\"Convert PDF to markdown content.\"\"\"\n",
                "    try:\n",
                "        pdf = converter.convert(pdf_path).document\n",
                "        return pdf.export_to_markdown()\n",
                "    except Exception as e:\n",
                "        print(f\"Error converting {pdf_path}: {str(e)}\")\n",
                "        return None\n",
                "\n",
                "\n",
                "# def split_content_by_headings(content, headings):\n",
                "#     \"\"\"Split content into ch unks based on headings.\"\"\"\n",
                "#     chunks = {}\n",
                "\n",
                "#     # Find all heading positions\n",
                "#     heading_positions = []\n",
                "#     for heading in headings:\n",
                "#         match = re.search(re.escape(heading), content, re.IGNORECASE | re.MULTILINE)\n",
                "#         if match:\n",
                "#             heading_positions.append((heading, match.start()))\n",
                "\n",
                "#     # Sort by position\n",
                "#     heading_positions.sort(key=lambda x: x[1])\n",
                "\n",
                "#     # Create chunks\n",
                "#     for i, (heading, start_pos) in enumerate(heading_positions):\n",
                "#         # Determine end position\n",
                "#         if i + 1 < len(heading_positions):\n",
                "#             end_pos = heading_positions[i + 1][1]\n",
                "#         else:\n",
                "#             end_pos = len(content)\n",
                "\n",
                "#         # Extract chunk\n",
                "#         chunk_content = content[start_pos:end_pos].strip()\n",
                "#         chunks[heading] = chunk_content\n",
                "\n",
                "#     return chunks\n",
                "\n",
                "\n",
                "def split_content_by_headings(content, headings):\n",
                "    \"\"\"Split content into chunks based on markdown headings (## ...).\"\"\"\n",
                "    chunks = {}\n",
                "\n",
                "    # Find all heading positions\n",
                "    heading_positions = []\n",
                "    for heading in headings:\n",
                "        # Match only if heading starts with ##\n",
                "        pattern = r\"^##\\s*\" + re.escape(heading) + r\"\\s*$\"\n",
                "        match = re.search(pattern, content, re.IGNORECASE | re.MULTILINE)\n",
                "        if match:\n",
                "            heading_positions.append((heading, match.start()))\n",
                "\n",
                "    # Sort by position\n",
                "    heading_positions.sort(key=lambda x: x[1])\n",
                "\n",
                "    # Create chunks\n",
                "    for i, (heading, start_pos) in enumerate(heading_positions):\n",
                "        if i + 1 < len(heading_positions):\n",
                "            end_pos = heading_positions[i + 1][1]\n",
                "        else:\n",
                "            end_pos = len(content)\n",
                "\n",
                "        chunk_content = content[start_pos:end_pos].strip()\n",
                "        chunks[heading] = chunk_content\n",
                "\n",
                "    return chunks\n",
                "\n",
                "\n",
                "def save_chunks(\n",
                "    chunks, output_folder, chapter_num, full_chapter_content=None, generate_summary=True\n",
                "):\n",
                "    \"\"\"Save chunks to separate files and optionally generate chapter summary.\"\"\"\n",
                "    chapter_output_folder = os.path.join(output_folder, f\"chapter_{chapter_num}\")\n",
                "    os.makedirs(chapter_output_folder, exist_ok=True)\n",
                "\n",
                "    chunk_info = {}\n",
                "\n",
                "    # Save individual chunks\n",
                "    for i, (heading, content) in enumerate(chunks.items(), 1):\n",
                "        # Create safe filename\n",
                "        safe_filename = re.sub(r\"[^\\w\\s-]\", \"\", heading)\n",
                "        safe_filename = re.sub(r\"[-\\s]+\", \"_\", safe_filename)\n",
                "        filename = f\"chunk_{i:02d}_{safe_filename[:50]}.md\"\n",
                "\n",
                "        # Save chunk\n",
                "        chunk_path = os.path.join(chapter_output_folder, filename)\n",
                "        with open(chunk_path, \"w\", encoding=\"utf-8\") as f:\n",
                "            f.write(f\"# {heading}\\n\\n\")\n",
                "            f.write(content)\n",
                "\n",
                "        chunk_info[heading] = {\n",
                "            \"filename\": filename,\n",
                "            \"word_count\": len(content.split()),\n",
                "            \"char_count\": len(content),\n",
                "        }\n",
                "\n",
                "    # Save chunk info\n",
                "    info_path = os.path.join(chapter_output_folder, \"chunk_info.json\")\n",
                "    with open(info_path, \"w\", encoding=\"utf-8\") as f:\n",
                "        json.dump(chunk_info, f, indent=2, ensure_ascii=False)\n",
                "\n",
                "    return chunk_info"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "id": "main_processing",
            "metadata": {},
            "outputs": [],
            "source": [
                "def process_folder(folder_path, generate_summaries=True):\n",
                "    \"\"\"Process a single folder containing PDFs and chapter structure.\"\"\"\n",
                "    folder_name = os.path.basename(folder_path)\n",
                "    print(f\"\\n=== Processing folder: {folder_name} ===\")\n",
                "\n",
                "    # Load chapter structure\n",
                "    chapter_structure = load_chapter_structure(folder_path)\n",
                "    if not chapter_structure:\n",
                "        return None\n",
                "\n",
                "    results = {}\n",
                "\n",
                "    # Process each chapter\n",
                "    for chapter_num, headings in chapter_structure.items():\n",
                "        pdf_path = os.path.join(folder_path, f\"{chapter_num}.pdf\")\n",
                "\n",
                "        if not os.path.exists(pdf_path):\n",
                "            print(f\"PDF not found: {pdf_path}\")\n",
                "            continue\n",
                "\n",
                "        print(f\"\\nProcessing Chapter {chapter_num}...\")\n",
                "        print(f\"Headings to extract: {len(headings)}\")\n",
                "\n",
                "        # Convert PDF to markdown\n",
                "        content = convert_pdf_to_markdown(pdf_path)\n",
                "        if not content:\n",
                "            print(f\"Failed to convert PDF: {pdf_path}\")\n",
                "            continue\n",
                "        print(f\"Total content length: {len(content)} characters\")\n",
                "\n",
                "        # Split into chunks\n",
                "        chunks = split_content_by_headings(content, headings)\n",
                "        print(f\"Successfully extracted {len(chunks)} chunks\")\n",
                "\n",
                "        if len(chunks) == 0:\n",
                "            print(\n",
                "                f\"No chunks found for Chapter {chapter_num}. Check if headings match the PDF content.\"\n",
                "            )\n",
                "            continue\n",
                "\n",
                "        # Save chunks with optional summary\n",
                "        output_folder = os.path.join(OUTPUT_PATH, folder_name)\n",
                "        chunk_info = save_chunks(\n",
                "            chunks, output_folder, chapter_num, content, generate_summaries\n",
                "        )\n",
                "\n",
                "        # Save Markdown content\n",
                "        markdown_folder = os.path.join(\n",
                "            OUTPUT_PATH, folder_name, f\"chapter_{chapter_num}\"\n",
                "        )\n",
                "        markdown_file = os.path.join(markdown_folder, f\"chapter_{chapter_num}.md\")\n",
                "        with open(markdown_file, \"w\", encoding=\"utf-8\") as f:\n",
                "            f.write(content)\n",
                "\n",
                "        # Track results\n",
                "        has_summary = \"chapter_summary\" in chunk_info and \"error\" not in chunk_info.get(\n",
                "            \"chapter_summary\", {}\n",
                "        )\n",
                "        results[chapter_num] = {\n",
                "            \"total_chunks\": len(chunks),\n",
                "            \"chunk_info\": chunk_info,\n",
                "            \"has_summary\": has_summary,\n",
                "        }\n",
                "\n",
                "        # Print chunk statistics\n",
                "        if has_summary:\n",
                "            summary_word_count = chunk_info[\"chapter_summary\"].get(\"word_count\", 0)\n",
                "            print(f\"    Chapter summary: {summary_word_count} words\")\n",
                "\n",
                "        for heading, info in chunk_info.items():\n",
                "            if heading != \"chapter_summary\":\n",
                "                print(\n",
                "                    f\"  - {heading[:60]}{'...' if len(heading) > 60 else ''}: {info['word_count']} words\"\n",
                "                )\n",
                "\n",
                "    return results\n",
                "\n",
                "\n",
                "def process_all_folders(base_path, generate_summaries=True):\n",
                "    \"\"\"Process all folders in the base path.\"\"\"\n",
                "    if not os.path.exists(base_path):\n",
                "        print(f\"Base path does not exist: {base_path}\")\n",
                "        return {}\n",
                "\n",
                "    all_results = {}\n",
                "    folders = [\n",
                "        item\n",
                "        for item in os.listdir(base_path)\n",
                "        if os.path.isdir(os.path.join(base_path, item))\n",
                "    ]\n",
                "\n",
                "    if not folders:\n",
                "        print(f\"No folders found in: {base_path}\")\n",
                "        return {}\n",
                "\n",
                "    print(f\"Found {len(folders)} folders to process: {folders}\")\n",
                "\n",
                "    for item in folders:\n",
                "        folder_path = os.path.join(base_path, item)\n",
                "        try:\n",
                "            results = process_folder(folder_path, generate_summaries)\n",
                "            if results:\n",
                "                all_results[item] = results\n",
                "        except Exception as e:\n",
                "            print(f\"Error processing folder {item}: {str(e)}\")\n",
                "\n",
                "    return all_results"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "id": "single_folder_example",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Processing single folder: ES_24-25\n",
                        "\n",
                        "=== Processing folder: ES_24-25 ===\n",
                        "\n",
                        "Processing Chapter 1...\n",
                        "Headings to extract: 6\n",
                        "Total content length: 55405 characters\n",
                        "Successfully extracted 6 chunks\n",
                        "  - STATE OF THE ECONOMY: GETTING BACK INTO THE FAST LANE: 350 words\n",
                        "  - Introduction: 137 words\n",
                        "  - Global Economic Scenario: 1965 words\n",
                        "  - Domestic Economy Remains Steady Amidst Global Uncertainties: 2444 words\n",
                        "  - Economy Characterised by Stability and Inclusivity on Multip...: 2873 words\n",
                        "  - Outlook and Way Forward: 423 words\n",
                        "\n",
                        "Processing Chapter 2...\n",
                        "Headings to extract: 6\n",
                        "Total content length: 123656 characters\n",
                        "Successfully extracted 5 chunks\n",
                        "  - Introduction: 244 words\n",
                        "  - Monetary Developments: 701 words\n",
                        "  - Financial Intermediation: 15015 words\n",
                        "  - Risks Pertaining to India's Financial Sector: 1246 words\n",
                        "  - Outlook: 604 words\n",
                        "\n",
                        "Processing Chapter 3...\n",
                        "Headings to extract: 7\n",
                        "Total content length: 93861 characters\n",
                        "Successfully extracted 7 chunks\n",
                        "  - EXTERNAL SECTOR: GETTING FDI RIGHT: 301 words\n",
                        "  - Introduction: 792 words\n",
                        "  - Global Trade Dynamics: 4675 words\n",
                        "  - Trend in India's Trade Performance: 4224 words\n",
                        "  - Ease of Doing Business Initiatives for Exporters: 380 words\n",
                        "  - Balance of Payments: Resilience Amid Challenges: 3207 words\n",
                        "  - Outlook: 294 words\n",
                        "\n",
                        "Processing Chapter 4...\n",
                        "Headings to extract: 5\n",
                        "Total content length: 26978 characters\n",
                        "Successfully extracted 4 chunks\n",
                        "  - Introduction: 248 words\n",
                        "  - Global Inflation: 339 words\n",
                        "  - Domestic Inflation: 2126 words\n",
                        "  - Outlook and Way Forward: 606 words\n",
                        "\n",
                        "Processing Chapter 5...\n",
                        "Headings to extract: 7\n",
                        "Total content length: 75554 characters\n",
                        "Successfully extracted 4 chunks\n",
                        "  - India's Medium-term Outlook: 2036 words\n",
                        "  - The Elephant and The Dragon in the Room: 577 words\n",
                        "  - Climate Transition, China and Geopolitics: 1956 words\n",
                        "  - Implications for India's Growth Prospects: 5987 words\n",
                        "\n",
                        "Processing Chapter 6...\n",
                        "Headings to extract: 11\n",
                        "Total content length: 56673 characters\n",
                        "Successfully extracted 10 chunks\n",
                        "  - Introduction: 450 words\n",
                        "  - Infrastructure Capex Improves Post-Election: 179 words\n",
                        "  - Physical Connectivity: 2130 words\n",
                        "  - Power Sector: 596 words\n",
                        "  - Digital Connectivity: 694 words\n",
                        "  - Rural Infrastructure: 1062 words\n",
                        "  - Urban Infrastructure: 1513 words\n",
                        "  - Tourism Infrastructure: 162 words\n",
                        "  - Space Infrastructure: 327 words\n",
                        "  - Conclusions and Way Forward: 505 words\n",
                        "\n",
                        "Processing Chapter 7...\n",
                        "Headings to extract: 9\n",
                        "Total content length: 62456 characters\n",
                        "Successfully extracted 7 chunks\n",
                        "  - INDUSTRY: ALL ABOUT BUSINESS REFORMS: 185 words\n",
                        "  - Global Background: 385 words\n",
                        "  - Recent Domestic Developments: 323 words\n",
                        "  - Core Input Industries: 591 words\n",
                        "  - Performance of Capital Goods and Consumer Goods Industries: 3778 words\n",
                        "  - Appropriate Policies Likely to Bring in Greater Equity in St...: 2337 words\n",
                        "  - Conclusion and Outlook: 426 words\n",
                        "\n",
                        "Processing Chapter 8...\n",
                        "Headings to extract: 9\n",
                        "Total content length: 46948 characters\n",
                        "Successfully extracted 8 chunks\n",
                        "  - SERVICES: NEW CHALLENGES FOR THE OLD WAR HORSE: 163 words\n",
                        "  - Introduction: 319 words\n",
                        "  - Services Sector Performance in India: 590 words\n",
                        "  - Sources of Financing: Bank Credit and FDI: 1493 words\n",
                        "  - Other Services: 432 words\n",
                        "  - Business Services: 1768 words\n",
                        "  - State Wise Analysis of Service Sector Performance: 1437 words\n",
                        "  - Conclusion and Way Forward: 495 words\n",
                        "\n",
                        "Processing Chapter 9...\n",
                        "Headings to extract: 15\n",
                        "Total content length: 61024 characters\n",
                        "Successfully extracted 14 chunks\n",
                        "  - AGRICULTURE AND FOOD MANAGEMENT: SECTOR OF THE FUTURE: 106 words\n",
                        "  - Introduction: 1375 words\n",
                        "  - Crop Production: Incentivising Productivity Increase, Crop D...: 284 words\n",
                        "  - Seeds-Quality and Use of Fertilisers: The Critical Different...: 293 words\n",
                        "  - Rainfall and Irrigation System: Building Efficiency and Exte...: 1566 words\n",
                        "  - Agriculture Credit: A Critical Input: 902 words\n",
                        "  - Agriculture Mechanisation: Facilitating Access: 263 words\n",
                        "  - Agriculture Extension: The Enabler: 243 words\n",
                        "  - Improvement in Agriculture Marketing Infrastructure: 782 words\n",
                        "  - Climate Action In Agriculture: 307 words\n",
                        "  - Allied Sectors: Potential to Build Resilience: 975 words\n",
                        "  - Food Processing Industries: Critical for the Economy: 291 words\n",
                        "  - Food Management: Enabling Food Security: 579 words\n",
                        "  - Conclusion: 639 words\n",
                        "\n",
                        "Processing Chapter 10...\n",
                        "Headings to extract: 8\n",
                        "Total content length: 79093 characters\n",
                        "Successfully extracted 8 chunks\n",
                        "  - CLIMATE AND ENVIRONMENT: ADAPTATION MATTERS: 166 words\n",
                        "  - Introduction: 1031 words\n",
                        "  - Bringing Adaptation to the Forefront: 2313 words\n",
                        "  - Energy Transition - Learning from the Experience of Develope...: 2640 words\n",
                        "  - Progress Made on India's Energy Transition: 2087 words\n",
                        "  - Optimising Lifestyles for Sustainable Development: 1581 words\n",
                        "  - Air Pollution: 342 words\n",
                        "  - Conclusion: 711 words\n",
                        "\n",
                        "Processing Chapter 11...\n",
                        "Headings to extract: 6\n",
                        "Total content length: 175210 characters\n",
                        "Successfully extracted 6 chunks\n",
                        "  - SOCIAL SECTOR: EXTENDING REACH AND DRIVING EMPOWERMENT: 146 words\n",
                        "  - Introduction: 2798 words\n",
                        "  - Education: Treading New Pathways: 8455 words\n",
                        "  - Towards A Healthy Nation: 7773 words\n",
                        "  - Rural Economy: 4256 words\n",
                        "  - Outlook: 529 words\n",
                        "\n",
                        "Processing Chapter 12...\n",
                        "Headings to extract: 6\n",
                        "Total content length: 114620 characters\n",
                        "Successfully extracted 6 chunks\n",
                        "  - EMPLOYMENT AND SKILL DEVELOPMENT: EXISTENTIAL PRIORITIES: 186 words\n",
                        "  - Introduction: 516 words\n",
                        "  - State of Employment: 6040 words\n",
                        "  - Job Creation: Action Towards Enhanced Employment Opportuniti...: 4540 words\n",
                        "  - Skill Development: Upskilling, Reskilling and New Skilling f...: 4450 words\n",
                        "  - Conclusion: 263 words\n",
                        "\n",
                        "Processing Chapter 13...\n",
                        "Headings to extract: 9\n",
                        "Total content length: 79061 characters\n",
                        "Successfully extracted 9 chunks\n",
                        "  - LABOUR IN THE AI ERA: CRISIS OR CATALYST?: 221 words\n",
                        "  - Introduction: 1165 words\n",
                        "  - Revolutions and Ripples: 1534 words\n",
                        "  - The Need for Robust Institutions: 1053 words\n",
                        "  - Vision to Viability: AI's Real World Challenges: 2395 words\n",
                        "  - AI and India: Are There Opportunities?: 1205 words\n",
                        "  - The Labour Market Evolution: 953 words\n",
                        "  - Augmenting India's Services Sector: 2308 words\n",
                        "  - Conclusion: 798 words\n",
                        "\n",
                        "=== Processing Complete ===\n",
                        "Results saved to: /workspace/output/\n",
                        "Processed 13 chapters\n"
                    ]
                }
            ],
            "source": [
                "# Example: Process a single folder\n",
                "single_folder_name = \"ES_24-25\"  # Change this to your folder name\n",
                "single_folder_path = os.path.join(BASE_PATH, single_folder_name)\n",
                "\n",
                "if os.path.exists(single_folder_path):\n",
                "    print(f\"Processing single folder: {single_folder_name}\")\n",
                "    results = process_folder(single_folder_path, generate_summaries=True)\n",
                "    if results:\n",
                "        print(\"\\n=== Processing Complete ===\")\n",
                "        print(f\"Results saved to: {OUTPUT_PATH}\")\n",
                "        print(f\"Processed {len(results)} chapters\")\n",
                "    else:\n",
                "        print(\"No results generated - check your folder structure and files\")\n",
                "else:\n",
                "    print(f\"Folder not found: {single_folder_path}\")\n",
                "    print(f\"Available folders in {BASE_PATH}:\")\n",
                "    if os.path.exists(BASE_PATH):\n",
                "        available_folders = [\n",
                "            f\n",
                "            for f in os.listdir(BASE_PATH)\n",
                "            if os.path.isdir(os.path.join(BASE_PATH, f))\n",
                "        ]\n",
                "        for folder in available_folders:\n",
                "            print(f\"  - {folder}\")\n",
                "    else:\n",
                "        print(f\"Base path does not exist: {BASE_PATH}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "id": "process_all",
            "metadata": {},
            "outputs": [],
            "source": [
                "# # Process all folders in the base path\n",
                "# print(\"Processing all folders...\")\n",
                "# all_results = process_all_folders(BASE_PATH, generate_summaries=True)\n",
                "\n",
                "# if all_results:\n",
                "#     # Save overall summary\n",
                "#     summary_path = os.path.join(OUTPUT_PATH, \"processing_summary.json\")\n",
                "#     with open(summary_path, \"w\", encoding=\"utf-8\") as f:\n",
                "#         json.dump(all_results, f, indent=2, ensure_ascii=False)\n",
                "\n",
                "#     print(\"\\n=== All Processing Complete ===\")\n",
                "#     print(f\"Summary saved to: {summary_path}\")\n",
                "#     print(f\"Processed {len(all_results)} folders\")\n",
                "# else:\n",
                "#     print(\"No folders were successfully processed\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "id": "inspection",
            "metadata": {},
            "outputs": [],
            "source": [
                "# # Inspect the results (only run if all_results exists)\n",
                "# if \"all_results\" in locals() and all_results:\n",
                "#     print(\"\\n=== Processing Summary ===\")\n",
                "#     for folder_name, folder_results in all_results.items():\n",
                "#         print(f\"\\nFolder: {folder_name}\")\n",
                "#         total_chunks = sum(\n",
                "#             chapter_data[\"total_chunks\"] for chapter_data in folder_results.values()\n",
                "#         )\n",
                "#         total_summaries = sum(\n",
                "#             1\n",
                "#             for chapter_data in folder_results.values()\n",
                "#             if chapter_data.get(\"has_summary\", False)\n",
                "#         )\n",
                "#         print(f\"  Total chapters processed: {len(folder_results)}\")\n",
                "#         print(f\"  Total chunks created: {total_chunks}\")\n",
                "#         print(f\"  Chapter summaries generated: {total_summaries}\")\n",
                "\n",
                "#         for chapter_num, chapter_data in folder_results.items():\n",
                "#             summary_status = \"✓\" if chapter_data.get(\"has_summary\", False) else \"✗\"\n",
                "#             print(\n",
                "#                 f\"    Chapter {chapter_num}: {chapter_data['total_chunks']} chunks, summary {summary_status}\"\n",
                "#             )\n",
                "# else:\n",
                "#     print(\"No results to inspect. Run the processing cells first.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "id": "summary_extraction",
            "metadata": {},
            "outputs": [],
            "source": [
                "# # Optional: Extract and consolidate all chapter summaries\n",
                "# def extract_all_summaries(base_output_path):\n",
                "#     \"\"\"Extract all chapter summaries from processed folders.\"\"\"\n",
                "#     if not os.path.exists(base_output_path):\n",
                "#         print(f\"Output path does not exist: {base_output_path}\")\n",
                "#         return {}\n",
                "\n",
                "#     all_summaries = {}\n",
                "\n",
                "#     for folder_name in os.listdir(base_output_path):\n",
                "#         folder_path = os.path.join(base_output_path, folder_name)\n",
                "#         if not os.path.isdir(folder_path):\n",
                "#             continue\n",
                "\n",
                "#         folder_summaries = {}\n",
                "\n",
                "#         for item in os.listdir(folder_path):\n",
                "#             if item.startswith(\"chapter_\") and os.path.isdir(\n",
                "#                 os.path.join(folder_path, item)\n",
                "#             ):\n",
                "#                 chapter_path = os.path.join(folder_path, item)\n",
                "#                 info_file = os.path.join(chapter_path, \"chunk_info.json\")\n",
                "\n",
                "#                 if os.path.exists(info_file):\n",
                "#                     try:\n",
                "#                         with open(info_file, \"r\", encoding=\"utf-8\") as f:\n",
                "#                             chunk_info = json.load(f)\n",
                "\n",
                "#                         if (\n",
                "#                             \"chapter_summary\" in chunk_info\n",
                "#                             and \"summary_text\" in chunk_info[\"chapter_summary\"]\n",
                "#                         ):\n",
                "#                             chapter_num = item.replace(\"chapter_\", \"\")\n",
                "#                             folder_summaries[chapter_num] = {\n",
                "#                                 \"summary\": chunk_info[\"chapter_summary\"][\n",
                "#                                     \"summary_text\"\n",
                "#                                 ],\n",
                "#                                 \"word_count\": chunk_info[\"chapter_summary\"].get(\n",
                "#                                     \"word_count\", 0\n",
                "#                                 ),\n",
                "#                             }\n",
                "#                     except Exception as e:\n",
                "#                         print(f\"Error reading {info_file}: {str(e)}\")\n",
                "\n",
                "#         if folder_summaries:\n",
                "#             all_summaries[folder_name] = folder_summaries\n",
                "\n",
                "#     return all_summaries\n",
                "\n",
                "\n",
                "# # Extract summaries and save to a consolidated file\n",
                "# summaries = extract_all_summaries(OUTPUT_PATH)\n",
                "\n",
                "# if summaries:\n",
                "#     # Save consolidated summaries\n",
                "#     summaries_path = os.path.join(OUTPUT_PATH, \"all_chapter_summaries.json\")\n",
                "#     with open(summaries_path, \"w\", encoding=\"utf-8\") as f:\n",
                "#         json.dump(summaries, f, indent=2, ensure_ascii=False)\n",
                "\n",
                "#     print(f\"\\n=== Chapter Summaries Consolidated ===\")\n",
                "#     print(f\"Summaries saved to: {summaries_path}\")\n",
                "\n",
                "#     # Display summary statistics\n",
                "#     for folder_name, folder_summaries in summaries.items():\n",
                "#         total_summary_words = sum(\n",
                "#             summary[\"word_count\"] for summary in folder_summaries.values()\n",
                "#         )\n",
                "#         print(\n",
                "#             f\"  {folder_name}: {len(folder_summaries)} chapters, {total_summary_words} total summary words\"\n",
                "#         )\n",
                "# else:\n",
                "#     print(\"No summaries found to consolidate. Process some chapters first.\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "tata",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.13.7"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
